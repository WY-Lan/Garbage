{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a3bc900",
   "metadata": {},
   "source": [
    "首先，需要安装deap(GA的标准库)和bitstring，这两个库用于训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "233cd8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from deap import base, creator, tools, algorithms\n",
    "from scipy.stats import bernoulli\n",
    "from bitstring import BitArray\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import optimizers\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b00427f",
   "metadata": {},
   "source": [
    "导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fc54442",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"aqi_eemd_out.csv\")\n",
    "Y=dataset.IMF1Y\n",
    "X=dataset.drop('IMF1Y',axis=1)\n",
    "data=np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b81d02",
   "metadata": {},
   "source": [
    "处理标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7e9098f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35163437]\n",
      " [0.5864258 ]\n",
      " [0.34064417]\n",
      " [0.54896157]\n",
      " [0.35297753]\n",
      " [0.60197102]\n",
      " [0.57834657]\n",
      " [0.24351036]\n",
      " [0.29895783]\n",
      " [0.38417486]\n",
      " [0.57553939]\n",
      " [0.34543667]\n",
      " [0.61603628]\n",
      " [0.39430006]\n",
      " [0.10116474]\n",
      " [0.8426602 ]\n",
      " [0.2697265 ]\n",
      " [0.17980671]\n",
      " [0.68274708]\n",
      " [0.34392951]\n",
      " [0.19817876]\n",
      " [0.46311312]\n",
      " [0.7870563 ]\n",
      " [0.57753368]\n",
      " [0.16863724]\n",
      " [0.40631618]\n",
      " [0.6128623 ]\n",
      " [0.33679032]\n",
      " [0.57446282]\n",
      " [0.31182459]\n",
      " [0.63644881]\n",
      " [0.6111173 ]\n",
      " [0.31431582]\n",
      " [0.16134979]\n",
      " [0.21252922]\n",
      " [0.53784315]\n",
      " [0.74143741]\n",
      " [0.72365571]\n",
      " [0.62234259]\n",
      " [0.42660093]\n",
      " [0.36792775]\n",
      " [0.53555629]\n",
      " [0.33976445]\n",
      " [0.40877419]\n",
      " [0.48526108]\n",
      " [0.80935822]\n",
      " [0.58658216]\n",
      " [0.00796409]\n",
      " [0.03838245]\n",
      " [0.13412085]\n",
      " [0.59445399]\n",
      " [0.8007033 ]\n",
      " [0.55174018]\n",
      " [0.08229919]\n",
      " [0.1278129 ]\n",
      " [0.57747506]\n",
      " [0.84071073]\n",
      " [0.16095775]\n",
      " [0.64392509]\n",
      " [0.31329258]\n",
      " [0.51463494]\n",
      " [0.70582551]\n",
      " [0.09518505]\n",
      " [0.10840647]\n",
      " [0.23414838]\n",
      " [0.36193291]\n",
      " [0.69320317]\n",
      " [0.41117673]\n",
      " [0.27403021]\n",
      " [0.66620405]\n",
      " [0.19110483]\n",
      " [0.37097652]\n",
      " [0.45826263]\n",
      " [0.73184956]\n",
      " [0.85703779]\n",
      " [0.86969814]\n",
      " [0.67258293]\n",
      " [0.60439085]\n",
      " [0.44694616]\n",
      " [0.38035986]\n",
      " [0.30124993]\n",
      " [0.42995021]\n",
      " [0.55215851]\n",
      " [0.33565698]\n",
      " [0.59802697]\n",
      " [0.68349916]\n",
      " [0.4341368 ]\n",
      " [0.21665841]\n",
      " [0.22635622]\n",
      " [0.56211907]\n",
      " [0.4072719 ]\n",
      " [0.49294266]\n",
      " [0.39313556]\n",
      " [0.54392345]\n",
      " [0.41476096]\n",
      " [0.40252177]\n",
      " [0.35867737]\n",
      " [0.57250386]\n",
      " [0.29325366]\n",
      " [0.68461741]\n",
      " [0.15765298]\n",
      " [0.36454005]\n",
      " [0.62085567]\n",
      " [0.85200481]\n",
      " [0.27056263]\n",
      " [0.14288437]\n",
      " [0.30928749]\n",
      " [0.60630566]\n",
      " [0.37320025]\n",
      " [0.49935183]\n",
      " [0.47314973]\n",
      " [0.42678864]\n",
      " [0.48871692]\n",
      " [0.40478769]\n",
      " [0.54884172]\n",
      " [0.30882477]\n",
      " [0.4395093 ]\n",
      " [0.53968939]\n",
      " [0.68345567]\n",
      " [0.73678193]\n",
      " [0.98933333]\n",
      " [0.98938909]\n",
      " [0.98944668]\n",
      " [0.98950605]\n",
      " [0.98956718]\n",
      " [0.98963003]\n",
      " [0.98969457]\n",
      " [0.98976077]\n",
      " [0.98982859]\n",
      " [0.98989801]\n",
      " [0.98996899]\n",
      " [0.9900415 ]\n",
      " [0.99011551]\n",
      " [0.99019099]\n",
      " [0.9902679 ]\n",
      " [0.99034621]\n",
      " [0.9904259 ]\n",
      " [0.99050693]\n",
      " [0.99058926]\n",
      " [0.99067287]\n",
      " [0.99075772]\n",
      " [0.99084379]\n",
      " [0.99093103]\n",
      " [0.99101942]\n",
      " [0.99110893]\n",
      " [0.99119952]\n",
      " [0.99129117]\n",
      " [0.99138383]\n",
      " [0.99147749]\n",
      " [0.9915721 ]\n",
      " [0.99166763]\n",
      " [0.99176406]\n",
      " [0.99186135]\n",
      " [0.99195947]\n",
      " [0.99205838]\n",
      " [0.99215807]\n",
      " [0.99225848]\n",
      " [0.9923596 ]\n",
      " [0.99246138]\n",
      " [0.99256381]\n",
      " [0.99266684]\n",
      " [0.99277044]\n",
      " [0.99287458]\n",
      " [0.99297924]\n",
      " [0.99308437]\n",
      " [0.99318996]\n",
      " [0.99329595]\n",
      " [0.99340233]\n",
      " [0.99350906]\n",
      " [0.9936161 ]\n",
      " [0.99372344]\n",
      " [0.99383103]\n",
      " [0.99393884]\n",
      " [0.99404685]\n",
      " [0.99415501]\n",
      " [0.9942633 ]\n",
      " [0.99437169]\n",
      " [0.99448014]\n",
      " [0.99458862]\n",
      " [0.9946971 ]\n",
      " [0.99480555]\n",
      " [0.99491394]\n",
      " [0.99502223]\n",
      " [0.9951304 ]\n",
      " [0.9952384 ]\n",
      " [0.99534621]\n",
      " [0.9954538 ]\n",
      " [0.99556114]\n",
      " [0.99566819]\n",
      " [0.99577491]\n",
      " [0.99588129]\n",
      " [0.99598729]\n",
      " [0.99609287]\n",
      " [0.996198  ]\n",
      " [0.99630266]\n",
      " [0.9964068 ]\n",
      " [0.99651041]\n",
      " [0.99661344]\n",
      " [0.99671586]\n",
      " [0.99681765]\n",
      " [0.99691876]\n",
      " [0.99701918]\n",
      " [0.99711886]\n",
      " [0.99721777]\n",
      " [0.99731589]\n",
      " [0.99741318]\n",
      " [0.99750961]\n",
      " [0.99760515]\n",
      " [0.99769976]\n",
      " [0.99779341]\n",
      " [0.99788608]\n",
      " [0.99797772]\n",
      " [0.99806831]\n",
      " [0.99815782]\n",
      " [0.99824621]\n",
      " [0.99833346]\n",
      " [0.99841952]\n",
      " [0.99850437]\n",
      " [0.99858798]\n",
      " [0.99867032]\n",
      " [0.99875134]\n",
      " [0.99883103]\n",
      " [0.99890934]\n",
      " [0.99898626]\n",
      " [0.99906173]\n",
      " [0.99913574]\n",
      " [0.99920825]\n",
      " [0.99927923]\n",
      " [0.99934865]\n",
      " [0.99941648]\n",
      " [0.99948267]\n",
      " [0.99954721]\n",
      " [0.99961006]\n",
      " [0.99967119]\n",
      " [0.99973056]\n",
      " [0.99978815]\n",
      " [0.99984392]\n",
      " [0.99989784]\n",
      " [0.99994988]\n",
      " [1.        ]\n",
      " [0.34598328]\n",
      " [0.34535392]\n",
      " [0.34488122]\n",
      " [0.34458036]\n",
      " [0.34446951]\n",
      " [0.34456682]\n",
      " [0.34489044]\n",
      " [0.34545852]\n",
      " [0.34628923]\n",
      " [0.34740071]\n",
      " [0.34881113]\n",
      " [0.35053862]\n",
      " [0.35260136]\n",
      " [0.35501749]\n",
      " [0.35780516]\n",
      " [0.36098254]\n",
      " [0.36456777]\n",
      " [0.36857188]\n",
      " [0.37297735]\n",
      " [0.37775953]\n",
      " [0.38289378]\n",
      " [0.38835544]\n",
      " [0.39411986]\n",
      " [0.4001624 ]\n",
      " [0.40645841]\n",
      " [0.41298324]\n",
      " [0.41971224]\n",
      " [0.42662076]\n",
      " [0.43368415]\n",
      " [0.44087776]\n",
      " [0.44817695]\n",
      " [0.4555566 ]\n",
      " [0.46298806]\n",
      " [0.47043567]\n",
      " [0.47786208]\n",
      " [0.48522999]\n",
      " [0.49250207]\n",
      " [0.49964099]\n",
      " [0.50660944]\n",
      " [0.51337008]\n",
      " [0.5198856 ]\n",
      " [0.52611867]\n",
      " [0.53203197]\n",
      " [0.53758818]\n",
      " [0.54274997]\n",
      " [0.54748002]\n",
      " [0.55174101]\n",
      " [0.55549561]\n",
      " [0.5587065 ]\n",
      " [0.56133636]\n",
      " [0.56334786]\n",
      " [0.56471431]\n",
      " [0.56545148]\n",
      " [0.56558579]\n",
      " [0.56514364]\n",
      " [0.56415143]\n",
      " [0.56263557]\n",
      " [0.56062246]\n",
      " [0.55813853]\n",
      " [0.55521015]\n",
      " [0.55186376]\n",
      " [0.54812574]\n",
      " [0.54402251]\n",
      " [0.53958048]\n",
      " [0.53482604]\n",
      " [0.5297856 ]\n",
      " [0.52448558]\n",
      " [0.51895237]\n",
      " [0.51321239]\n",
      " [0.50729189]\n",
      " [0.50121663]\n",
      " [0.49501218]\n",
      " [0.48870416]\n",
      " [0.48231815]\n",
      " [0.47587976]\n",
      " [0.46941458]\n",
      " [0.46294821]\n",
      " [0.45650625]\n",
      " [0.45011429]\n",
      " [0.44379794]\n",
      " [0.43758278]\n",
      " [0.43149443]\n",
      " [0.42555846]\n",
      " [0.41980049]\n",
      " [0.41424611]\n",
      " [0.40892091]\n",
      " [0.40384687]\n",
      " [0.3990314 ]\n",
      " [0.39447665]\n",
      " [0.39017809]\n",
      " [0.38612955]\n",
      " [0.38232485]\n",
      " [0.37875783]\n",
      " [0.3754223 ]\n",
      " [0.37231208]\n",
      " [0.36942101]\n",
      " [0.3667429 ]\n",
      " [0.36427158]\n",
      " [0.36200088]\n",
      " [0.35992462]\n",
      " [0.35803662]\n",
      " [0.35633071]\n",
      " [0.3548007 ]\n",
      " [0.35344044]\n",
      " [0.35224373]\n",
      " [0.35120441]\n",
      " [0.35031629]\n",
      " [0.34957321]\n",
      " [0.34896899]\n",
      " [0.34849744]\n",
      " [0.3481524 ]\n",
      " [0.34792769]\n",
      " [0.34781714]\n",
      " [0.34781456]\n",
      " [0.34791378]\n",
      " [0.34810863]\n",
      " [0.34839293]\n",
      " [0.34876051]\n",
      " [0.34920518]\n",
      " [0.34972185]\n",
      " [0.39065316]\n",
      " [0.37215293]\n",
      " [0.35573223]\n",
      " [0.4168849 ]\n",
      " [0.51795355]\n",
      " [0.52373244]\n",
      " [0.45541782]\n",
      " [0.4108679 ]\n",
      " [0.46023715]\n",
      " [0.53033436]\n",
      " [0.51371408]\n",
      " [0.35704163]\n",
      " [0.22715832]\n",
      " [0.35486087]\n",
      " [0.6274873 ]\n",
      " [0.82529262]\n",
      " [0.71266117]\n",
      " [0.4060924 ]\n",
      " [0.13330324]\n",
      " [0.11277485]\n",
      " [0.28648618]\n",
      " [0.52230366]\n",
      " [0.6766334 ]\n",
      " [0.64774644]\n",
      " [0.51337093]\n",
      " [0.38679454]\n",
      " [0.34408579]\n",
      " [0.4174094 ]\n",
      " [0.51241511]\n",
      " [0.56379175]\n",
      " [0.5790622 ]\n",
      " [0.56997535]\n",
      " [0.55004511]\n",
      " [0.52209751]\n",
      " [0.48513176]\n",
      " [0.43564449]\n",
      " [0.36950669]\n",
      " [0.28780216]\n",
      " [0.21246579]\n",
      " [0.17064525]\n",
      " [0.19172215]\n",
      " [0.30632887]\n",
      " [0.49986127]\n",
      " [0.69159038]\n",
      " [0.8179839 ]\n",
      " [0.82399134]\n",
      " [0.6777975 ]\n",
      " [0.44052857]\n",
      " [0.19654604]\n",
      " [0.02816489]\n",
      " [0.        ]\n",
      " [0.1354492 ]\n",
      " [0.42342786]\n",
      " [0.75316593]\n",
      " [0.95095839]\n",
      " [0.92011561]\n",
      " [0.6185186 ]\n",
      " [0.20457389]\n",
      " [0.04952287]\n",
      " [0.27016327]\n",
      " [0.5602538 ]\n",
      " [0.71959564]\n",
      " [0.65503271]\n",
      " [0.48933943]\n",
      " [0.37082132]\n",
      " [0.33397816]\n",
      " [0.38338935]\n",
      " [0.49898806]\n",
      " [0.58415618]\n",
      " [0.55372866]\n",
      " [0.44991324]\n",
      " [0.35290229]\n",
      " [0.27675629]\n",
      " [0.2173831 ]\n",
      " [0.17069061]\n",
      " [0.13160321]\n",
      " [0.09539958]\n",
      " [0.07407489]\n",
      " [0.08661833]\n",
      " [0.15105585]\n",
      " [0.27160211]\n",
      " [0.44093119]\n",
      " [0.61988434]\n",
      " [0.74727786]\n",
      " [0.73409645]\n",
      " [0.59656795]\n",
      " [0.41170441]\n",
      " [0.30238991]\n",
      " [0.35783676]\n",
      " [0.49105611]\n",
      " [0.56715676]\n",
      " [0.50058977]\n",
      " [0.38973302]\n",
      " [0.33007769]\n",
      " [0.33795943]\n",
      " [0.41447459]\n",
      " [0.51605214]\n",
      " [0.58831483]\n",
      " [0.58350118]\n",
      " [0.4865302 ]\n",
      " [0.38529107]\n",
      " [0.35825325]\n",
      " [0.3936521 ]\n",
      " [0.45356839]\n",
      " [0.50483419]\n",
      " [0.53854005]\n",
      " [0.54912266]\n",
      " [0.51408899]\n",
      " [0.41806286]\n",
      " [0.30701816]\n",
      " [0.29668976]\n",
      " [0.41583426]\n",
      " [0.58474234]\n",
      " [0.66201311]\n",
      " [0.58125777]\n",
      " [0.43552434]\n",
      " [0.3214431 ]\n",
      " [0.25368621]\n",
      " [0.22733342]\n",
      " [0.23718096]\n",
      " [0.59567742]\n",
      " [0.61265494]\n",
      " [0.61833357]\n",
      " [0.61046333]\n",
      " [0.5850383 ]\n",
      " [0.53927169]\n",
      " [0.47785441]\n",
      " [0.41051682]\n",
      " [0.34865985]\n",
      " [0.30698539]\n",
      " [0.29947438]\n",
      " [0.3303919 ]\n",
      " [0.38833272]\n",
      " [0.45805408]\n",
      " [0.52308723]\n",
      " [0.56665693]\n",
      " [0.57672617]\n",
      " [0.55984598]\n",
      " [0.52584522]\n",
      " [0.48418768]\n",
      " [0.44380008]\n",
      " [0.41146088]\n",
      " [0.39341146]\n",
      " [0.39388624]\n",
      " [0.40902435]\n",
      " [0.43268819]\n",
      " [0.45864373]\n",
      " [0.48122102]\n",
      " [0.49744206]\n",
      " [0.50522323]\n",
      " [0.50348359]\n",
      " [0.49233685]\n",
      " [0.47362874]\n",
      " [0.44960145]\n",
      " [0.42249716]\n",
      " [0.39458212]\n",
      " [0.36821876]\n",
      " [0.34579354]\n",
      " [0.32969293]\n",
      " [0.32232407]\n",
      " [0.32556028]\n",
      " [0.33882914]\n",
      " [0.36094166]\n",
      " [0.39070885]\n",
      " [0.42694171]\n",
      " [0.46845124]\n",
      " [0.51355108]\n",
      " [0.55856546]\n",
      " [0.59932123]\n",
      " [0.63164524]\n",
      " [0.65136435]\n",
      " [0.65523108]\n",
      " [0.64355829]\n",
      " [0.61701495]\n",
      " [0.57612764]\n",
      " [0.52276698]\n",
      " [0.46417958]\n",
      " [0.4089561 ]\n",
      " [0.36568716]\n",
      " [0.34127612]\n",
      " [0.33587719]\n",
      " [0.34797461]\n",
      " [0.37511025]\n",
      " [0.41078398]\n",
      " [0.44743243]\n",
      " [0.47747927]\n",
      " [0.49498675]\n",
      " [0.50057122]\n",
      " [0.4966214 ]\n",
      " [0.48593383]\n",
      " [0.47092971]\n",
      " [0.45390293]\n",
      " [0.4371474 ]\n",
      " [0.42296451]\n",
      " [0.41368566]\n",
      " [0.41158504]\n",
      " [0.41867802]\n",
      " [0.43636837]\n",
      " [0.46335412]\n",
      " [0.49571417]\n",
      " [0.52900934]\n",
      " [0.55880048]\n",
      " [0.58064843]\n",
      " [0.59011402]\n",
      " [0.58376448]\n",
      " [0.5621926 ]\n",
      " [0.52699754]\n",
      " [0.4797785 ]\n",
      " [0.42270401]\n",
      " [0.36143059]\n",
      " [0.30702573]\n",
      " [0.2701704 ]\n",
      " [0.25515791]\n",
      " [0.26475075]\n",
      " [0.29998897]\n",
      " [0.35403081]\n",
      " [0.41804754]\n",
      " [0.4832104 ]\n",
      " [0.54069068]\n",
      " [0.58294539]\n",
      " [0.60757468]\n",
      " [0.6134948 ]\n",
      " [0.60090744]\n",
      " [0.5747008 ]\n",
      " [0.54092711]\n",
      " [0.50518051]\n",
      " [0.4712227 ]\n",
      " [0.44199957]\n",
      " [0.41902607]\n",
      " [0.40345947]\n",
      " [0.396457  ]\n",
      " [0.39850253]\n",
      " [0.40736967]\n",
      " [0.42009164]\n",
      " [0.43397672]\n",
      " [0.44750029]\n",
      " [0.45942952]\n",
      " [0.46853158]\n",
      " [0.47357365]\n",
      " [0.47367148]\n",
      " [0.35200997]\n",
      " [0.34896104]\n",
      " [0.3511361 ]\n",
      " [0.3588944 ]\n",
      " [0.37163789]\n",
      " [0.38852994]\n",
      " [0.40873391]\n",
      " [0.43141318]\n",
      " [0.45573111]\n",
      " [0.48085107]\n",
      " [0.50590043]\n",
      " [0.52984166]\n",
      " [0.55151756]\n",
      " [0.56975003]\n",
      " [0.58336095]\n",
      " [0.59117223]\n",
      " [0.59200574]\n",
      " [0.58505562]\n",
      " [0.57100491]\n",
      " [0.55090887]\n",
      " [0.5258228 ]\n",
      " [0.49680196]\n",
      " [0.46490163]\n",
      " [0.43117708]\n",
      " [0.39675101]\n",
      " [0.36301586]\n",
      " [0.33146051]\n",
      " [0.30368998]\n",
      " [0.28133832]\n",
      " [0.26582542]\n",
      " [0.25756852]\n",
      " [0.25618711]\n",
      " [0.26115821]\n",
      " [0.27196589]\n",
      " [0.28809583]\n",
      " [0.3090097 ]\n",
      " [0.33407297]\n",
      " [0.36262706]\n",
      " [0.3940134 ]\n",
      " [0.42755274]\n",
      " [0.46247071]\n",
      " [0.49792277]\n",
      " [0.53305201]\n",
      " [0.56700152]\n",
      " [0.59891439]\n",
      " [0.62793372]\n",
      " [0.6532026 ]\n",
      " [0.67386411]\n",
      " [0.68906135]\n",
      " [0.69793741]\n",
      " [0.69963539]\n",
      " [0.69347717]\n",
      " [0.67964233]\n",
      " [0.65905875]\n",
      " [0.63279674]\n",
      " [0.60192658]\n",
      " [0.56751857]\n",
      " [0.53064299]\n",
      " [0.49237014]\n",
      " [0.45377031]\n",
      " [0.41591379]\n",
      " [0.37985357]\n",
      " [0.34656588]\n",
      " [0.31699257]\n",
      " [0.29211965]\n",
      " [0.27294604]\n",
      " [0.26030106]\n",
      " [0.25433554]\n",
      " [0.25489694]\n",
      " [0.26129768]\n",
      " [0.27271696]\n",
      " [0.2883341 ]\n",
      " [0.30732842]\n",
      " [0.32887174]\n",
      " [0.35210591]\n",
      " [0.37622995]\n",
      " [0.40070171]\n",
      " [0.42504373]\n",
      " [0.44877856]\n",
      " [0.47142874]\n",
      " [0.49251682]\n",
      " [0.51156534]\n",
      " [0.52809685]\n",
      " [0.54163388]\n",
      " [0.55183841]\n",
      " [0.55893008]\n",
      " [0.56327159]\n",
      " [0.56524017]\n",
      " [0.56521836]\n",
      " [0.56359534]\n",
      " [0.56076193]\n",
      " [0.557109  ]\n",
      " [0.55302736]\n",
      " [0.54884173]\n",
      " [0.54461233]\n",
      " [0.54033322]\n",
      " [0.53599848]\n",
      " [0.5316022 ]\n",
      " [0.52713844]\n",
      " [0.5226013 ]\n",
      " [0.51798483]\n",
      " [0.51325278]\n",
      " [0.50828221]\n",
      " [0.50305876]\n",
      " [0.49760279]\n",
      " [0.49193467]\n",
      " [0.48607478]\n",
      " [0.48004348]\n",
      " [0.47386115]\n",
      " [0.46754815]\n",
      " [0.46112486]\n",
      " [0.4546323 ]\n",
      " [0.44821093]\n",
      " [0.44208886]\n",
      " [0.43651094]\n",
      " [0.43172203]\n",
      " [0.42796699]\n",
      " [0.42549067]\n",
      " [0.42453793]]\n"
     ]
    }
   ],
   "source": [
    "#convert Y(output label) to numpy array\n",
    "y = Y.to_numpy()\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "encoded_y = min_max_scaler.fit_transform(data)\n",
    "print(encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "952a3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_rmse(y_test, yhat):\n",
    "    return (((y_test - yhat)**2).mean())**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d70fc676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):\n",
    "    #开始进行解码，将神经元的数量，epoch还有batchsize 这些进行处理\n",
    "    # Decode GA solution to integer for num_neurons1, epochs and batch_size \n",
    "    #在内部进行初始化，否则回报错\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_train_scaled = min_max_scaler.fit_transform(data)\n",
    "    \n",
    "    num_neurons1_bits = BitArray(ga_individual_solution[0:9])\n",
    "    num_neurons2_bits = BitArray(ga_individual_solution[9:18])\n",
    "    epoch_bits = BitArray(ga_individual_solution[18:25]) \n",
    "    batch_size_bits = BitArray(ga_individual_solution[25:35])\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[35:])\n",
    "   \n",
    "    num_neurons1 = num_neurons1_bits.uint\n",
    "    num_neurons2 = num_neurons2_bits.uint\n",
    "    epoch = epoch_bits.uint\n",
    "    Batch_size = batch_size_bits.uint\n",
    "    temp = learning_rate_bits.uint\n",
    "    learning_rate = temp*(math.exp(-9))\n",
    "    \n",
    "    print('\\nNum of neurons1: ', num_neurons1,'\\nNum of neurons2',num_neurons2, '\\nEpoch:', epoch,'\\nBatch size:',Batch_size,'\\nLearning rate:',learning_rate)\n",
    "    \n",
    "    # Return fitness score of 100 if window_size or num_unit is zero\n",
    "    if num_neurons1 < 100 or num_neurons2 < 100 or epoch < 100 or Batch_size < 500  or learning_rate < 0:\n",
    "      return 0,\n",
    "    \n",
    "    #split into train and validation (80/20)\n",
    "    x_train,x_test,y_train,y_test=train_test_split(x_train_scaled,encoded_y,test_size=0.2)\n",
    "    \n",
    "    #converting the input train and test set to array format\n",
    "    x_train=np.array(x_train)\n",
    "    x_test=np.array(x_test)\n",
    "    \n",
    "    #reshape input data according to LSTM model requirements\n",
    "    x_train_modified = x_train.reshape(x_train.shape[0],1,x_train.shape[1])\n",
    "    x_test_modified = x_test.reshape(x_test.shape[0],1,x_test.shape[1])\n",
    "    \n",
    "    #Design the LSTM model\n",
    "    #LSTM的优化方法，这里可以注意一下，按照Adam的优化方法进行计算\n",
    "    optimizer=optimizers.adam_v2.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(CuDNNLSTM(num_neurons1, input_shape=(x_train_modified.shape[1],x_train_modified.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(CuDNNLSTM(num_neurons2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y_train.shape[1]))\n",
    "    model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    history=model.fit(x_train_modified, y_train, epochs=epoch, batch_size=Batch_size,verbose=0)\n",
    "    y_pred = model.predict(x_test_modified)\n",
    "    print(y_pred)\n",
    "    rmse = cal_rmse(y_test, y_pred)\n",
    "    \n",
    "    return [rmse]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e389652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\deap\\creator.py:138: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "D:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\deap\\creator.py:138: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "D:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2023-02-26 19:03:02.184127\n",
      "\n",
      "Num of neurons1:  152 \n",
      "Num of neurons2 185 \n",
      "Epoch: 62 \n",
      "Batch size: 277 \n",
      "Learning rate: 0.012094160800494597\n",
      "\n",
      "Num of neurons1:  86 \n",
      "Num of neurons2 263 \n",
      "Epoch: 65 \n",
      "Batch size: 315 \n",
      "Learning rate: 0.007898227461547492\n",
      "\n",
      "Num of neurons1:  64 \n",
      "Num of neurons2 382 \n",
      "Epoch: 101 \n",
      "Batch size: 750 \n",
      "Learning rate: 0.00839186667789421\n",
      "\n",
      "Num of neurons1:  511 \n",
      "Num of neurons2 272 \n",
      "Epoch: 125 \n",
      "Batch size: 273 \n",
      "Learning rate: 0.0003702294122600387\n",
      "\n",
      "Num of neurons1:  107 \n",
      "Num of neurons2 385 \n",
      "Epoch: 117 \n",
      "Batch size: 320 \n",
      "Learning rate: 0.007898227461547492\n",
      "\n",
      "Num of neurons1:  33 \n",
      "Num of neurons2 310 \n",
      "Epoch: 29 \n",
      "Batch size: 585 \n",
      "Learning rate: 0.007404588245200774\n",
      "\n",
      "Num of neurons1:  395 \n",
      "Num of neurons2 115 \n",
      "Epoch: 73 \n",
      "Batch size: 955 \n",
      "Learning rate: 0.015179405902661586\n",
      "\n",
      "Num of neurons1:  394 \n",
      "Num of neurons2 375 \n",
      "Epoch: 89 \n",
      "Batch size: 420 \n",
      "Learning rate: 0.006540719616594017\n",
      "\n",
      "Num of neurons1:  76 \n",
      "Num of neurons2 477 \n",
      "Epoch: 86 \n",
      "Batch size: 310 \n",
      "Learning rate: 0.012958029429101355\n",
      "\n",
      "Num of neurons1:  265 \n",
      "Num of neurons2 275 \n",
      "Epoch: 121 \n",
      "Batch size: 108 \n",
      "Learning rate: 0.0013575078449534752\n",
      "\n",
      "Num of neurons1:  506 \n",
      "Num of neurons2 438 \n",
      "Epoch: 43 \n",
      "Batch size: 229 \n",
      "Learning rate: 0.0007404588245200774\n",
      "\n",
      "Num of neurons1:  97 \n",
      "Num of neurons2 338 \n",
      "Epoch: 42 \n",
      "Batch size: 621 \n",
      "Learning rate: 0.013698488253621431\n",
      "\n",
      "Num of neurons1:  415 \n",
      "Num of neurons2 53 \n",
      "Epoch: 90 \n",
      "Batch size: 234 \n",
      "Learning rate: 0.004319343143033784\n",
      "\n",
      "Num of neurons1:  60 \n",
      "Num of neurons2 157 \n",
      "Epoch: 4 \n",
      "Batch size: 663 \n",
      "Learning rate: 0.00012340980408667956\n",
      "\n",
      "Num of neurons1:  226 \n",
      "Num of neurons2 460 \n",
      "Epoch: 91 \n",
      "Batch size: 502 \n",
      "Learning rate: 0.009625964718761006\n",
      "\n",
      "Num of neurons1:  140 \n",
      "Num of neurons2 53 \n",
      "Epoch: 121 \n",
      "Batch size: 739 \n",
      "Learning rate: 0.0025916058858202707\n",
      "\n",
      "Num of neurons1:  95 \n",
      "Num of neurons2 286 \n",
      "Epoch: 34 \n",
      "Batch size: 933 \n",
      "Learning rate: 0.00839186667789421\n",
      "\n",
      "Num of neurons1:  178 \n",
      "Num of neurons2 42 \n",
      "Epoch: 102 \n",
      "Batch size: 977 \n",
      "Learning rate: 0.00851527648198089\n",
      "\n",
      "Num of neurons1:  198 \n",
      "Num of neurons2 50 \n",
      "Epoch: 88 \n",
      "Batch size: 519 \n",
      "Learning rate: 0.0027150156899069505\n",
      "\n",
      "Num of neurons1:  172 \n",
      "Num of neurons2 72 \n",
      "Epoch: 5 \n",
      "Batch size: 246 \n",
      "Learning rate: 0.005430031379813901\n",
      "\n",
      "Num of neurons1:  91 \n",
      "Num of neurons2 175 \n",
      "Epoch: 33 \n",
      "Batch size: 904 \n",
      "Learning rate: 0.013081439233188033\n",
      "\n",
      "Num of neurons1:  131 \n",
      "Num of neurons2 468 \n",
      "Epoch: 34 \n",
      "Batch size: 476 \n",
      "Learning rate: 0.0076514078533741325\n",
      "\n",
      "Num of neurons1:  88 \n",
      "Num of neurons2 397 \n",
      "Epoch: 20 \n",
      "Batch size: 115 \n",
      "Learning rate: 0.010243013739194404\n",
      "\n",
      "Num of neurons1:  112 \n",
      "Num of neurons2 395 \n",
      "Epoch: 109 \n",
      "Batch size: 123 \n",
      "Learning rate: 0.001110688236780116\n",
      "\n",
      "Num of neurons1:  264 \n",
      "Num of neurons2 177 \n",
      "Epoch: 79 \n",
      "Batch size: 182 \n",
      "Learning rate: 0.014562356882228188\n",
      "\n",
      "Num of neurons1:  64 \n",
      "Num of neurons2 491 \n",
      "Epoch: 116 \n",
      "Batch size: 775 \n",
      "Learning rate: 0.015055996098574907\n",
      "\n",
      "Num of neurons1:  471 \n",
      "Num of neurons2 317 \n",
      "Epoch: 44 \n",
      "Batch size: 496 \n",
      "Learning rate: 0.0056768509879872595\n",
      "\n",
      "Num of neurons1:  128 \n",
      "Num of neurons2 353 \n",
      "Epoch: 27 \n",
      "Batch size: 341 \n",
      "Learning rate: 0.003085245102166989\n",
      "\n",
      "Num of neurons1:  394 \n",
      "Num of neurons2 410 \n",
      "Epoch: 37 \n",
      "Batch size: 567 \n",
      "Learning rate: 0.013698488253621431\n",
      "\n",
      "Num of neurons1:  343 \n",
      "Num of neurons2 164 \n",
      "Epoch: 116 \n",
      "Batch size: 921 \n",
      "Learning rate: 0.012217570604581276\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cu_dnnlstm_98 (CuDNNLSTM)   (None, 1, 343)            474712    \n",
      "                                                                 \n",
      " dropout_98 (Dropout)        (None, 1, 343)            0         \n",
      "                                                                 \n",
      " cu_dnnlstm_99 (CuDNNLSTM)   (None, 164)               333904    \n",
      "                                                                 \n",
      " dropout_99 (Dropout)        (None, 164)               0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 1)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 808,781\n",
      "Trainable params: 808,781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[0.56754625]\n",
      " [0.54897493]\n",
      " [0.56534976]\n",
      " [0.63065237]\n",
      " [0.36341658]\n",
      " [0.40599597]\n",
      " [1.0154185 ]\n",
      " [0.3384808 ]\n",
      " [0.47403297]\n",
      " [0.39850178]\n",
      " [0.37608886]\n",
      " [0.58568865]\n",
      " [0.5394454 ]\n",
      " [1.0091604 ]\n",
      " [0.34442788]\n",
      " [0.40525728]\n",
      " [0.35009715]\n",
      " [0.3461658 ]\n",
      " [0.30344957]\n",
      " [0.3891862 ]\n",
      " [0.39108834]\n",
      " [1.0184102 ]\n",
      " [0.30937225]\n",
      " [0.33441016]\n",
      " [0.3579614 ]\n",
      " [0.42053396]\n",
      " [0.3508003 ]\n",
      " [0.33186218]\n",
      " [0.57748836]\n",
      " [0.46160477]\n",
      " [0.40290985]\n",
      " [0.44848266]\n",
      " [0.5450737 ]\n",
      " [0.49046218]\n",
      " [0.380223  ]\n",
      " [0.38691607]\n",
      " [0.3467077 ]\n",
      " [0.43660653]\n",
      " [0.500492  ]\n",
      " [0.43090567]\n",
      " [1.0190891 ]\n",
      " [0.5242076 ]\n",
      " [1.0202006 ]\n",
      " [0.10934215]\n",
      " [0.3642756 ]\n",
      " [0.7250378 ]\n",
      " [0.34751764]\n",
      " [0.34103435]\n",
      " [0.57288086]\n",
      " [1.0140411 ]\n",
      " [0.5095044 ]\n",
      " [1.0120592 ]\n",
      " [1.0115831 ]\n",
      " [0.5258647 ]\n",
      " [1.019357  ]\n",
      " [0.51974815]\n",
      " [1.0093433 ]\n",
      " [0.5214912 ]\n",
      " [0.35434452]\n",
      " [1.0139152 ]\n",
      " [0.355486  ]\n",
      " [0.5031064 ]\n",
      " [0.47884008]\n",
      " [0.36173683]\n",
      " [0.563763  ]\n",
      " [0.3873462 ]\n",
      " [0.5222091 ]\n",
      " [0.5630147 ]\n",
      " [0.82188106]\n",
      " [0.2953029 ]\n",
      " [0.41090596]\n",
      " [0.4771823 ]\n",
      " [0.6823005 ]\n",
      " [0.42191306]\n",
      " [0.35085532]\n",
      " [0.81531245]\n",
      " [0.45995486]\n",
      " [1.013664  ]\n",
      " [0.6678899 ]\n",
      " [0.52627593]\n",
      " [0.536495  ]\n",
      " [0.53826874]\n",
      " [0.2748555 ]\n",
      " [0.60492045]\n",
      " [0.7278857 ]\n",
      " [0.30759543]\n",
      " [0.59080315]\n",
      " [0.5008264 ]\n",
      " [0.5852786 ]\n",
      " [0.51738304]\n",
      " [1.0196096 ]\n",
      " [0.32425323]\n",
      " [0.3837981 ]\n",
      " [1.0124229 ]\n",
      " [1.0096294 ]\n",
      " [0.85269445]\n",
      " [0.49252254]\n",
      " [0.53829384]\n",
      " [1.0192695 ]\n",
      " [0.36548433]\n",
      " [0.43191722]\n",
      " [0.3969429 ]\n",
      " [1.016986  ]\n",
      " [1.0103455 ]\n",
      " [0.3796074 ]\n",
      " [0.66597396]\n",
      " [0.2764445 ]\n",
      " [0.2909294 ]\n",
      " [0.46206427]\n",
      " [1.0141667 ]\n",
      " [0.18908371]\n",
      " [0.48686686]\n",
      " [0.545167  ]\n",
      " [0.8402353 ]\n",
      " [1.0188069 ]\n",
      " [0.3993525 ]\n",
      " [1.0086517 ]\n",
      " [0.33194804]\n",
      " [0.36943212]\n",
      " [0.48736063]\n",
      " [0.722657  ]\n",
      " [0.31653675]\n",
      " [0.21101779]\n",
      " [0.46612853]\n",
      " [0.4240555 ]\n",
      " [0.4712732 ]\n",
      " [1.0119392 ]\n",
      " [0.34504998]\n",
      " [0.3894595 ]\n",
      " [0.5274952 ]\n",
      " [1.0145439 ]\n",
      " [0.4744721 ]\n",
      " [1.0127914 ]\n",
      " [1.0204467 ]\n",
      " [0.48714712]\n",
      " [0.6184692 ]\n",
      " [0.44289914]\n",
      " [0.34226313]\n",
      " [0.40340334]\n",
      " [0.24600041]\n",
      " [0.39606556]\n",
      " [0.42590648]\n",
      " [0.27447066]\n",
      " [0.5049928 ]]\n",
      "\n",
      "Num of neurons1:  281 \n",
      "Num of neurons2 61 \n",
      "Epoch: 99 \n",
      "Batch size: 850 \n",
      "Learning rate: 0.0020979666694735527\n",
      "\n",
      "Num of neurons1:  488 \n",
      "Num of neurons2 375 \n",
      "Epoch: 66 \n",
      "Batch size: 206 \n",
      "Learning rate: 0.003332064710340348\n",
      "\n",
      "Num of neurons1:  7 \n",
      "Num of neurons2 192 \n",
      "Epoch: 61 \n",
      "Batch size: 590 \n",
      "Learning rate: 0.0071577686370274144\n",
      "\n",
      "Num of neurons1:  62 \n",
      "Num of neurons2 8 \n",
      "Epoch: 40 \n",
      "Batch size: 156 \n",
      "Learning rate: 0.006540719616594017\n",
      "\n",
      "Num of neurons1:  357 \n",
      "Num of neurons2 21 \n",
      "Epoch: 119 \n",
      "Batch size: 488 \n",
      "Learning rate: 0.001110688236780116\n",
      "\n",
      "Num of neurons1:  207 \n",
      "Num of neurons2 318 \n",
      "Epoch: 78 \n",
      "Batch size: 946 \n",
      "Learning rate: 0.01406871766588147\n",
      "\n",
      "Num of neurons1:  470 \n",
      "Num of neurons2 110 \n",
      "Epoch: 32 \n",
      "Batch size: 625 \n",
      "Learning rate: 0.012711209820927995\n",
      "\n",
      "Num of neurons1:  190 \n",
      "Num of neurons2 278 \n",
      "Epoch: 112 \n",
      "Batch size: 940 \n",
      "Learning rate: 0.011600521584147878\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cu_dnnlstm_100 (CuDNNLSTM)  (None, 1, 190)            146680    \n",
      "                                                                 \n",
      " dropout_100 (Dropout)       (None, 1, 190)            0         \n",
      "                                                                 \n",
      " cu_dnnlstm_101 (CuDNNLSTM)  (None, 278)               522640    \n",
      "                                                                 \n",
      " dropout_101 (Dropout)       (None, 278)               0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 1)                 279       \n",
      "                                                                 \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 669,599\n",
      "Trainable params: 669,599\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57457227]\n",
      " [0.54001814]\n",
      " [0.46128398]\n",
      " [0.45119956]\n",
      " [0.40514436]\n",
      " [0.62953085]\n",
      " [0.3567779 ]\n",
      " [0.35185567]\n",
      " [0.3957637 ]\n",
      " [1.0057749 ]\n",
      " [1.0067849 ]\n",
      " [0.410713  ]\n",
      " [0.9983757 ]\n",
      " [0.53594804]\n",
      " [0.27053627]\n",
      " [0.8655376 ]\n",
      " [0.5061282 ]\n",
      " [0.24032359]\n",
      " [0.5457684 ]\n",
      " [0.57105446]\n",
      " [0.29441002]\n",
      " [0.47606736]\n",
      " [0.49521768]\n",
      " [0.81640375]\n",
      " [0.48120818]\n",
      " [1.0091535 ]\n",
      " [0.4541352 ]\n",
      " [0.44914547]\n",
      " [0.32751524]\n",
      " [0.50705826]\n",
      " [0.38568196]\n",
      " [1.0009838 ]\n",
      " [0.6298993 ]\n",
      " [0.59361315]\n",
      " [1.0054245 ]\n",
      " [0.55415636]\n",
      " [1.0015637 ]\n",
      " [0.9992782 ]\n",
      " [0.4755007 ]\n",
      " [0.61343396]\n",
      " [0.3125747 ]\n",
      " [0.6641236 ]\n",
      " [0.48691595]\n",
      " [0.16330099]\n",
      " [0.44511944]\n",
      " [0.26458144]\n",
      " [0.33048275]\n",
      " [0.18496764]\n",
      " [1.0065665 ]\n",
      " [1.0025192 ]\n",
      " [1.0069991 ]\n",
      " [0.9976421 ]\n",
      " [0.6337405 ]\n",
      " [1.008185  ]\n",
      " [1.0033714 ]\n",
      " [0.2496776 ]\n",
      " [0.439904  ]\n",
      " [0.3047246 ]\n",
      " [0.34856454]\n",
      " [1.0073127 ]\n",
      " [0.51494   ]\n",
      " [0.3594129 ]\n",
      " [0.46748033]\n",
      " [0.5696955 ]\n",
      " [0.5029106 ]\n",
      " [0.3328052 ]\n",
      " [0.52803904]\n",
      " [0.49351895]\n",
      " [0.35587612]\n",
      " [0.55213517]\n",
      " [1.0089351 ]\n",
      " [0.05984868]\n",
      " [0.41678235]\n",
      " [0.55380243]\n",
      " [0.70690423]\n",
      " [0.33864543]\n",
      " [0.59647995]\n",
      " [0.3318462 ]\n",
      " [0.34027004]\n",
      " [0.36602595]\n",
      " [0.3983049 ]\n",
      " [0.34779456]\n",
      " [0.42539737]\n",
      " [1.0094781 ]\n",
      " [0.41119894]\n",
      " [1.0092224 ]\n",
      " [1.0056586 ]\n",
      " [0.5136718 ]\n",
      " [0.382988  ]\n",
      " [0.35212794]\n",
      " [0.31524584]\n",
      " [0.13495961]\n",
      " [0.5175478 ]\n",
      " [1.0021576 ]\n",
      " [0.3603126 ]\n",
      " [0.42659843]\n",
      " [0.5116578 ]\n",
      " [0.26401553]\n",
      " [0.20054233]\n",
      " [0.38609022]\n",
      " [0.46387607]\n",
      " [0.26619592]\n",
      " [0.49996558]\n",
      " [0.48876628]\n",
      " [0.4864573 ]\n",
      " [0.6691579 ]\n",
      " [1.0078107 ]\n",
      " [0.35659674]\n",
      " [1.0084498 ]\n",
      " [0.51369333]\n",
      " [1.0090827 ]\n",
      " [0.2325992 ]\n",
      " [1.0087003 ]\n",
      " [1.0041038 ]\n",
      " [0.2652984 ]\n",
      " [0.3453825 ]\n",
      " [0.72188747]\n",
      " [0.80999994]\n",
      " [0.47291446]\n",
      " [0.38961133]\n",
      " [0.52306896]\n",
      " [0.49681923]\n",
      " [0.41237488]\n",
      " [0.3042682 ]\n",
      " [0.42955628]\n",
      " [0.4720677 ]\n",
      " [0.36960852]\n",
      " [0.48159114]\n",
      " [0.35849005]\n",
      " [1.0043468 ]\n",
      " [1.0028833 ]\n",
      " [0.33942804]\n",
      " [0.4590188 ]\n",
      " [0.3068551 ]\n",
      " [0.63738185]\n",
      " [0.44149983]\n",
      " [0.5489517 ]\n",
      " [0.40826932]\n",
      " [0.45379025]\n",
      " [0.9998858 ]\n",
      " [0.34416392]\n",
      " [0.9988993 ]\n",
      " [0.4007335 ]\n",
      " [0.50169665]]\n",
      "\n",
      "Num of neurons1:  54 \n",
      "Num of neurons2 319 \n",
      "Epoch: 38 \n",
      "Batch size: 165 \n",
      "Learning rate: 0.0027150156899069505\n",
      "\n",
      "Num of neurons1:  137 \n",
      "Num of neurons2 191 \n",
      "Epoch: 15 \n",
      "Batch size: 195 \n",
      "Learning rate: 0.005059801967553862\n",
      "\n",
      "Num of neurons1:  61 \n",
      "Num of neurons2 20 \n",
      "Epoch: 84 \n",
      "Batch size: 349 \n",
      "Learning rate: 0.0009872784326934365\n",
      "\n",
      "Num of neurons1:  279 \n",
      "Num of neurons2 481 \n",
      "Epoch: 31 \n",
      "Batch size: 202 \n",
      "Learning rate: 0.002468196081733591\n",
      "\n",
      "Num of neurons1:  106 \n",
      "Num of neurons2 345 \n",
      "Epoch: 93 \n",
      "Batch size: 123 \n",
      "Learning rate: 0.011353701975974519\n",
      "\n",
      "Num of neurons1:  180 \n",
      "Num of neurons2 247 \n",
      "Epoch: 103 \n",
      "Batch size: 206 \n",
      "Learning rate: 0.009379145110587647\n",
      "\n",
      "Num of neurons1:  195 \n",
      "Num of neurons2 219 \n",
      "Epoch: 65 \n",
      "Batch size: 231 \n",
      "Learning rate: 0.009996194131021045\n",
      "\n",
      "Num of neurons1:  16 \n",
      "Num of neurons2 395 \n",
      "Epoch: 26 \n",
      "Batch size: 765 \n",
      "Learning rate: 0.011723931388234559\n",
      "\n",
      "Num of neurons1:  5 \n",
      "Num of neurons2 91 \n",
      "Epoch: 17 \n",
      "Batch size: 330 \n",
      "Learning rate: 0.004812982359380503\n",
      "\n",
      "Num of neurons1:  293 \n",
      "Num of neurons2 357 \n",
      "Epoch: 13 \n",
      "Batch size: 73 \n",
      "Learning rate: 0.004442752947120464\n",
      "\n",
      "Num of neurons1:  68 \n",
      "Num of neurons2 504 \n",
      "Epoch: 119 \n",
      "Batch size: 998 \n",
      "Learning rate: 0.00012340980408667956\n",
      "\n",
      "Num of neurons1:  241 \n",
      "Num of neurons2 232 \n",
      "Epoch: 95 \n",
      "Batch size: 511 \n",
      "Learning rate: 0.013081439233188033\n",
      "\n",
      "Num of neurons1:  156 \n",
      "Num of neurons2 285 \n",
      "Epoch: 89 \n",
      "Batch size: 412 \n",
      "Learning rate: 0.013204849037274712\n",
      "\n",
      "Num of neurons1:  318 \n",
      "Num of neurons2 278 \n",
      "Epoch: 116 \n",
      "Batch size: 921 \n",
      "Learning rate: 0.012217570604581276\n",
      "Model: \"sequential_51\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cu_dnnlstm_102 (CuDNNLSTM)  (None, 1, 318)            408312    \n",
      "                                                                 \n",
      " dropout_102 (Dropout)       (None, 1, 318)            0         \n",
      "                                                                 \n",
      " cu_dnnlstm_103 (CuDNNLSTM)  (None, 278)               664976    \n",
      "                                                                 \n",
      " dropout_103 (Dropout)       (None, 278)               0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 1)                 279       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,073,567\n",
      "Trainable params: 1,073,567\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[0.44053492]\n",
      " [0.30859917]\n",
      " [0.38946712]\n",
      " [0.3297591 ]\n",
      " [0.35055542]\n",
      " [0.5648188 ]\n",
      " [0.57936096]\n",
      " [0.4189869 ]\n",
      " [0.3725392 ]\n",
      " [0.32560688]\n",
      " [0.4456946 ]\n",
      " [0.34788805]\n",
      " [0.2777055 ]\n",
      " [1.01364   ]\n",
      " [0.45055982]\n",
      " [0.41246328]\n",
      " [0.3152985 ]\n",
      " [1.0059632 ]\n",
      " [0.33489603]\n",
      " [0.3386839 ]\n",
      " [0.58728385]\n",
      " [0.5660005 ]\n",
      " [0.68438137]\n",
      " [0.4304225 ]\n",
      " [0.31044754]\n",
      " [0.34471163]\n",
      " [0.55479187]\n",
      " [0.17855553]\n",
      " [0.4199954 ]\n",
      " [0.7293639 ]\n",
      " [0.3130199 ]\n",
      " [0.525697  ]\n",
      " [0.5742891 ]\n",
      " [1.0149918 ]\n",
      " [0.55928445]\n",
      " [0.569909  ]\n",
      " [0.38609487]\n",
      " [0.36888215]\n",
      " [0.4733593 ]\n",
      " [1.0067469 ]\n",
      " [0.3359407 ]\n",
      " [0.65634966]\n",
      " [0.21671893]\n",
      " [0.47194138]\n",
      " [0.4118663 ]\n",
      " [1.0061827 ]\n",
      " [0.44238797]\n",
      " [0.04921421]\n",
      " [0.40474433]\n",
      " [0.66288537]\n",
      " [1.0043699 ]\n",
      " [0.10424437]\n",
      " [0.43356398]\n",
      " [1.0037122 ]\n",
      " [0.5726148 ]\n",
      " [0.3294052 ]\n",
      " [1.0036378 ]\n",
      " [0.5500032 ]\n",
      " [0.5122601 ]\n",
      " [0.5730176 ]\n",
      " [1.0058548 ]\n",
      " [1.012208  ]\n",
      " [0.5545822 ]\n",
      " [0.14993872]\n",
      " [0.49036565]\n",
      " [0.36873373]\n",
      " [0.4529411 ]\n",
      " [0.11483275]\n",
      " [0.30799994]\n",
      " [0.5384694 ]\n",
      " [0.63750947]\n",
      " [1.0086514 ]\n",
      " [0.35090402]\n",
      " [0.6451364 ]\n",
      " [0.37923405]\n",
      " [0.46450916]\n",
      " [0.6520626 ]\n",
      " [1.0096267 ]\n",
      " [0.40563622]\n",
      " [0.235994  ]\n",
      " [0.704237  ]\n",
      " [1.007925  ]\n",
      " [0.5496379 ]\n",
      " [0.04267275]\n",
      " [0.41188082]\n",
      " [0.4474665 ]\n",
      " [0.30559278]\n",
      " [0.5157294 ]\n",
      " [0.4690473 ]\n",
      " [1.0124259 ]\n",
      " [0.5424861 ]\n",
      " [0.43225887]\n",
      " [1.013824  ]\n",
      " [1.0146475 ]\n",
      " [1.0032321 ]\n",
      " [0.58582157]\n",
      " [0.43471947]\n",
      " [0.53764427]\n",
      " [0.45508233]\n",
      " [0.45587558]\n",
      " [0.50800407]\n",
      " [0.55199856]\n",
      " [0.27032036]\n",
      " [0.24562638]\n",
      " [0.66704196]\n",
      " [0.27682117]\n",
      " [0.55406046]\n",
      " [0.5727547 ]\n",
      " [1.007566  ]\n",
      " [0.33949584]\n",
      " [0.40977132]\n",
      " [0.3934183 ]\n",
      " [0.49107397]\n",
      " [0.68243474]\n",
      " [0.51599747]\n",
      " [0.35833353]\n",
      " [0.2236858 ]\n",
      " [0.45485076]\n",
      " [0.34622738]\n",
      " [0.26262525]\n",
      " [0.4969327 ]\n",
      " [0.87105036]\n",
      " [0.43361497]\n",
      " [0.9603542 ]\n",
      " [0.5590606 ]\n",
      " [1.0103536 ]\n",
      " [0.48817098]\n",
      " [0.43988803]\n",
      " [0.48633686]\n",
      " [0.3613918 ]\n",
      " [0.50898075]\n",
      " [0.47743377]\n",
      " [0.851875  ]\n",
      " [0.1333371 ]\n",
      " [0.5416273 ]\n",
      " [0.37823758]\n",
      " [0.72655743]\n",
      " [0.3425302 ]\n",
      " [0.1949774 ]\n",
      " [0.5775623 ]\n",
      " [0.5169972 ]\n",
      " [0.53835106]\n",
      " [0.40306076]\n",
      " [0.35579824]]\n",
      "\n",
      "Num of neurons1:  51 \n",
      "Num of neurons2 244 \n",
      "Epoch: 120 \n",
      "Batch size: 966 \n",
      "Learning rate: 0.008762096090154249\n",
      "\n",
      "Num of neurons1:  414 \n",
      "Num of neurons2 300 \n",
      "Epoch: 115 \n",
      "Batch size: 226 \n",
      "Learning rate: 0.01123029217188784\n",
      "\n",
      "Num of neurons1:  190 \n",
      "Num of neurons2 278 \n",
      "Epoch: 112 \n",
      "Batch size: 940 \n",
      "Learning rate: 0.011600521584147878\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cu_dnnlstm_104 (CuDNNLSTM)  (None, 1, 190)            146680    \n",
      "                                                                 \n",
      " dropout_104 (Dropout)       (None, 1, 190)            0         \n",
      "                                                                 \n",
      " cu_dnnlstm_105 (CuDNNLSTM)  (None, 278)               522640    \n",
      "                                                                 \n",
      " dropout_105 (Dropout)       (None, 278)               0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 1)                 279       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,599\n",
      "Trainable params: 669,599\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3468016 ]\n",
      " [0.4003879 ]\n",
      " [0.18213774]\n",
      " [0.16032186]\n",
      " [0.7299735 ]\n",
      " [0.34223166]\n",
      " [0.2796266 ]\n",
      " [0.45267   ]\n",
      " [0.3885306 ]\n",
      " [0.4602085 ]\n",
      " [0.7779722 ]\n",
      " [0.04978925]\n",
      " [0.32562014]\n",
      " [0.36547264]\n",
      " [1.0003654 ]\n",
      " [0.4040176 ]\n",
      " [0.68001246]\n",
      " [0.3059908 ]\n",
      " [0.2786685 ]\n",
      " [0.4771589 ]\n",
      " [0.43293476]\n",
      " [0.5245842 ]\n",
      " [0.38414887]\n",
      " [0.4688862 ]\n",
      " [0.40687436]\n",
      " [0.46281603]\n",
      " [0.34281003]\n",
      " [1.0035713 ]\n",
      " [0.34606653]\n",
      " [0.3458497 ]\n",
      " [1.0008003 ]\n",
      " [0.4045085 ]\n",
      " [0.33473662]\n",
      " [0.59013766]\n",
      " [0.5536273 ]\n",
      " [0.5071566 ]\n",
      " [1.0107486 ]\n",
      " [0.54340494]\n",
      " [0.3431038 ]\n",
      " [0.51599723]\n",
      " [0.3668979 ]\n",
      " [0.9997598 ]\n",
      " [0.9564253 ]\n",
      " [0.3299784 ]\n",
      " [0.47105652]\n",
      " [0.43146738]\n",
      " [0.50528824]\n",
      " [0.5920465 ]\n",
      " [0.41476285]\n",
      " [0.43184593]\n",
      " [0.3089383 ]\n",
      " [0.46965656]\n",
      " [0.53814805]\n",
      " [0.50929725]\n",
      " [0.29230437]\n",
      " [0.1268434 ]\n",
      " [0.4275589 ]\n",
      " [0.20800124]\n",
      " [0.31458965]\n",
      " [0.4187245 ]\n",
      " [0.5394069 ]\n",
      " [0.36410514]\n",
      " [0.30896756]\n",
      " [0.38862813]\n",
      " [0.41929618]\n",
      " [0.62226343]\n",
      " [1.0099504 ]\n",
      " [0.35231373]\n",
      " [0.48764247]\n",
      " [0.51992357]\n",
      " [1.010045  ]\n",
      " [0.21903682]\n",
      " [0.4526322 ]\n",
      " [1.0032196 ]\n",
      " [0.58666825]\n",
      " [0.34803393]\n",
      " [0.54506344]\n",
      " [0.5015548 ]\n",
      " [0.5382682 ]\n",
      " [0.53855664]\n",
      " [1.0005344 ]\n",
      " [0.31099942]\n",
      " [0.33982512]\n",
      " [0.5469879 ]\n",
      " [0.44799736]\n",
      " [0.5814673 ]\n",
      " [0.40982184]\n",
      " [0.6391944 ]\n",
      " [0.8353556 ]\n",
      " [0.4215407 ]\n",
      " [1.0028738 ]\n",
      " [0.18831564]\n",
      " [0.40201622]\n",
      " [0.53558755]\n",
      " [0.5102557 ]\n",
      " [0.5270095 ]\n",
      " [0.81092644]\n",
      " [1.007097  ]\n",
      " [0.31075063]\n",
      " [0.8187629 ]\n",
      " [1.0078065 ]\n",
      " [1.0023143 ]\n",
      " [0.6930085 ]\n",
      " [1.0051439 ]\n",
      " [0.50119287]\n",
      " [1.0112737 ]\n",
      " [0.35011324]\n",
      " [0.3790244 ]\n",
      " [0.48599282]\n",
      " [0.22589529]\n",
      " [1.0068562 ]\n",
      " [0.34409592]\n",
      " [0.4746446 ]\n",
      " [0.39036214]\n",
      " [0.42869923]\n",
      " [0.48736483]\n",
      " [1.0024244 ]\n",
      " [0.24463977]\n",
      " [0.41647658]\n",
      " [0.4253146 ]\n",
      " [0.190021  ]\n",
      " [0.4777667 ]\n",
      " [0.4806365 ]\n",
      " [1.0111331 ]\n",
      " [1.0036898 ]\n",
      " [0.4838737 ]\n",
      " [0.55327135]\n",
      " [0.5007477 ]\n",
      " [0.3995003 ]\n",
      " [1.0013716 ]\n",
      " [0.34735814]\n",
      " [0.3474933 ]\n",
      " [0.41500622]\n",
      " [0.35087726]\n",
      " [0.9999    ]\n",
      " [1.0069767 ]\n",
      " [0.35771745]\n",
      " [0.3521162 ]\n",
      " [0.35485506]\n",
      " [0.4686335 ]\n",
      " [0.5488274 ]\n",
      " [0.3655511 ]\n",
      " [0.5722901 ]\n",
      " [0.52758825]]\n",
      "\n",
      "Num of neurons1:  190 \n",
      "Num of neurons2 278 \n",
      "Epoch: 112 \n",
      "Batch size: 940 \n",
      "Learning rate: 0.011600521584147878\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cu_dnnlstm_106 (CuDNNLSTM)  (None, 1, 190)            146680    \n",
      "                                                                 \n",
      " dropout_106 (Dropout)       (None, 1, 190)            0         \n",
      "                                                                 \n",
      " cu_dnnlstm_107 (CuDNNLSTM)  (None, 278)               522640    \n",
      "                                                                 \n",
      " dropout_107 (Dropout)       (None, 278)               0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 1)                 279       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,599\n",
      "Trainable params: 669,599\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43440533]\n",
      " [0.51144934]\n",
      " [0.35663998]\n",
      " [0.5119009 ]\n",
      " [0.29242936]\n",
      " [0.58476603]\n",
      " [0.40530735]\n",
      " [0.45861757]\n",
      " [0.21225572]\n",
      " [0.8226703 ]\n",
      " [0.3955215 ]\n",
      " [0.50275886]\n",
      " [0.5402819 ]\n",
      " [0.27286518]\n",
      " [1.0131108 ]\n",
      " [0.47362375]\n",
      " [0.48566455]\n",
      " [1.0084984 ]\n",
      " [1.013567  ]\n",
      " [1.0103226 ]\n",
      " [1.0048519 ]\n",
      " [0.59452176]\n",
      " [1.0128777 ]\n",
      " [0.49551365]\n",
      " [0.55585915]\n",
      " [0.2912532 ]\n",
      " [0.26212847]\n",
      " [0.45248783]\n",
      " [1.0138998 ]\n",
      " [1.0154073 ]\n",
      " [0.48201922]\n",
      " [1.0127603 ]\n",
      " [0.58387554]\n",
      " [0.17333193]\n",
      " [0.34400716]\n",
      " [0.50818604]\n",
      " [1.0074632 ]\n",
      " [0.5783184 ]\n",
      " [0.34071675]\n",
      " [1.0098293 ]\n",
      " [0.46397477]\n",
      " [0.42552167]\n",
      " [1.0140088 ]\n",
      " [0.41339204]\n",
      " [0.7816236 ]\n",
      " [0.35513875]\n",
      " [0.55350876]\n",
      " [0.05535074]\n",
      " [0.4097451 ]\n",
      " [1.0157549 ]\n",
      " [0.2779919 ]\n",
      " [0.38401476]\n",
      " [1.0122843 ]\n",
      " [1.0059214 ]\n",
      " [1.0105698 ]\n",
      " [0.41174653]\n",
      " [1.0097065 ]\n",
      " [1.0064039 ]\n",
      " [1.0120431 ]\n",
      " [0.34746072]\n",
      " [0.24760985]\n",
      " [1.009584  ]\n",
      " [0.36856687]\n",
      " [1.0124042 ]\n",
      " [0.35875714]\n",
      " [0.503093  ]\n",
      " [0.57296294]\n",
      " [0.36541483]\n",
      " [0.4948164 ]\n",
      " [0.25316662]\n",
      " [1.0052284 ]\n",
      " [1.0061102 ]\n",
      " [0.40305957]\n",
      " [0.6659622 ]\n",
      " [0.6730292 ]\n",
      " [1.01474   ]\n",
      " [1.0158377 ]\n",
      " [0.49686188]\n",
      " [0.49035636]\n",
      " [0.4789197 ]\n",
      " [0.44548687]\n",
      " [1.0083804 ]\n",
      " [0.51928526]\n",
      " [0.54037774]\n",
      " [0.31146374]\n",
      " [0.351837  ]\n",
      " [0.45609024]\n",
      " [0.6106605 ]\n",
      " [1.016151  ]\n",
      " [0.42068136]\n",
      " [0.52367777]\n",
      " [0.5160267 ]\n",
      " [0.42431903]\n",
      " [0.50909436]\n",
      " [0.3140017 ]\n",
      " [0.36308357]\n",
      " [0.35871956]\n",
      " [0.3596305 ]\n",
      " [0.3117141 ]\n",
      " [0.44152677]\n",
      " [0.5582923 ]\n",
      " [0.45426443]\n",
      " [0.3934718 ]\n",
      " [1.0100756 ]\n",
      " [0.60110915]\n",
      " [0.5128418 ]\n",
      " [0.2303157 ]\n",
      " [1.0148396 ]\n",
      " [0.5957957 ]\n",
      " [0.37422016]\n",
      " [0.3472166 ]\n",
      " [1.0067096 ]\n",
      " [0.3489762 ]\n",
      " [0.34669003]\n",
      " [0.33080947]\n",
      " [0.5248254 ]\n",
      " [0.23883824]\n",
      " [0.4470535 ]\n",
      " [0.15370952]\n",
      " [0.16013658]\n",
      " [0.36205348]\n",
      " [0.34328076]\n",
      " [0.72843444]\n",
      " [0.41874042]\n",
      " [0.44182268]\n",
      " [0.6163082 ]\n",
      " [0.3651468 ]\n",
      " [0.23813774]\n",
      " [0.11937042]\n",
      " [1.0162961 ]\n",
      " [1.016366  ]\n",
      " [0.39184055]\n",
      " [0.549745  ]\n",
      " [0.42107153]\n",
      " [0.36795682]\n",
      " [0.38972387]\n",
      " [0.60916036]\n",
      " [0.40501827]\n",
      " [0.30904925]\n",
      " [0.571727  ]\n",
      " [0.46438062]\n",
      " [0.3895596 ]\n",
      " [0.53861266]\n",
      " [1.0121639 ]]\n",
      "\n",
      "Num of neurons1:  190 \n",
      "Num of neurons2 278 \n",
      "Epoch: 116 \n",
      "Batch size: 940 \n",
      "Learning rate: 0.011600521584147878\n",
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cu_dnnlstm_108 (CuDNNLSTM)  (None, 1, 190)            146680    \n",
      "                                                                 \n",
      " dropout_108 (Dropout)       (None, 1, 190)            0         \n",
      "                                                                 \n",
      " cu_dnnlstm_109 (CuDNNLSTM)  (None, 278)               522640    \n",
      "                                                                 \n",
      " dropout_109 (Dropout)       (None, 278)               0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 1)                 279       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,599\n",
      "Trainable params: 669,599\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0082057 ]\n",
      " [0.2782771 ]\n",
      " [0.347983  ]\n",
      " [0.5033079 ]\n",
      " [0.6793091 ]\n",
      " [0.5035969 ]\n",
      " [0.4532769 ]\n",
      " [0.57992405]\n",
      " [0.31308126]\n",
      " [0.4027055 ]\n",
      " [1.0160775 ]\n",
      " [1.0139157 ]\n",
      " [0.34840643]\n",
      " [0.4177043 ]\n",
      " [0.551172  ]\n",
      " [0.71102804]\n",
      " [0.36030376]\n",
      " [0.34434724]\n",
      " [0.46473128]\n",
      " [1.0068196 ]\n",
      " [0.3949535 ]\n",
      " [0.80626035]\n",
      " [0.67472124]\n",
      " [0.4626127 ]\n",
      " [0.38504338]\n",
      " [0.46308878]\n",
      " [0.5168002 ]\n",
      " [0.49553755]\n",
      " [0.5613672 ]\n",
      " [0.3872409 ]\n",
      " [0.5097441 ]\n",
      " [0.37027958]\n",
      " [0.14001787]\n",
      " [0.5807555 ]\n",
      " [0.41158187]\n",
      " [0.37528944]\n",
      " [0.40911704]\n",
      " [0.47349966]\n",
      " [0.45243338]\n",
      " [0.64847565]\n",
      " [0.8423478 ]\n",
      " [0.3314469 ]\n",
      " [0.81556827]\n",
      " [0.31105465]\n",
      " [0.540973  ]\n",
      " [0.46518865]\n",
      " [0.48188448]\n",
      " [0.42196074]\n",
      " [0.4126687 ]\n",
      " [0.5199155 ]\n",
      " [0.5408772 ]\n",
      " [0.39039892]\n",
      " [0.16071296]\n",
      " [0.5421421 ]\n",
      " [0.6073359 ]\n",
      " [0.5649594 ]\n",
      " [0.34892362]\n",
      " [1.012067  ]\n",
      " [0.44610554]\n",
      " [0.5034216 ]\n",
      " [0.3449264 ]\n",
      " [0.620591  ]\n",
      " [1.0177286 ]\n",
      " [1.0182812 ]\n",
      " [0.49687734]\n",
      " [0.51254505]\n",
      " [0.3503603 ]\n",
      " [0.3199989 ]\n",
      " [1.0080019 ]\n",
      " [1.0066626 ]\n",
      " [0.26507065]\n",
      " [0.26434913]\n",
      " [1.0084141 ]\n",
      " [1.018341  ]\n",
      " [0.42382276]\n",
      " [1.0065129 ]\n",
      " [1.0096376 ]\n",
      " [0.2735044 ]\n",
      " [0.52724934]\n",
      " [0.28169322]\n",
      " [0.52699995]\n",
      " [1.0085204 ]\n",
      " [0.38718933]\n",
      " [0.47629082]\n",
      " [0.33070496]\n",
      " [0.5655533 ]\n",
      " [0.19044705]\n",
      " [0.37620872]\n",
      " [0.30690637]\n",
      " [0.27954757]\n",
      " [1.0065869 ]\n",
      " [0.6514972 ]\n",
      " [0.40172467]\n",
      " [0.39600348]\n",
      " [0.4290637 ]\n",
      " [0.346744  ]\n",
      " [0.51773405]\n",
      " [0.7262116 ]\n",
      " [0.37721813]\n",
      " [0.52793103]\n",
      " [0.34788236]\n",
      " [0.42497534]\n",
      " [0.3491459 ]\n",
      " [0.4040137 ]\n",
      " [0.09190719]\n",
      " [0.5963331 ]\n",
      " [0.34571713]\n",
      " [1.0159724 ]\n",
      " [0.42639562]\n",
      " [0.55310583]\n",
      " [1.0124401 ]\n",
      " [1.0153182 ]\n",
      " [0.38223332]\n",
      " [0.4168325 ]\n",
      " [0.43983755]\n",
      " [0.53003585]\n",
      " [0.3897427 ]\n",
      " [0.14363377]\n",
      " [0.4252735 ]\n",
      " [0.32400408]\n",
      " [1.0178787 ]\n",
      " [1.0097542 ]\n",
      " [1.0067402 ]\n",
      " [0.51238394]\n",
      " [1.0157586 ]\n",
      " [0.36290428]\n",
      " [0.49106425]\n",
      " [0.547174  ]\n",
      " [0.5523123 ]\n",
      " [1.0109543 ]\n",
      " [0.28053245]\n",
      " [0.53387827]\n",
      " [0.349481  ]\n",
      " [1.0126883 ]\n",
      " [0.23217146]\n",
      " [0.48568976]\n",
      " [0.68849427]\n",
      " [0.5555814 ]\n",
      " [0.38506436]\n",
      " [0.4654106 ]\n",
      " [0.516663  ]\n",
      " [0.29774302]\n",
      " [0.47132143]\n",
      " [0.33971733]]\n",
      "\n",
      "Num of neurons1:  343 \n",
      "Num of neurons2 164 \n",
      "Epoch: 112 \n",
      "Batch size: 921 \n",
      "Learning rate: 0.012217570604581276\n",
      "Model: \"sequential_55\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cu_dnnlstm_110 (CuDNNLSTM)  (None, 1, 343)            474712    \n",
      "                                                                 \n",
      " dropout_110 (Dropout)       (None, 1, 343)            0         \n",
      "                                                                 \n",
      " cu_dnnlstm_111 (CuDNNLSTM)  (None, 164)               333904    \n",
      "                                                                 \n",
      " dropout_111 (Dropout)       (None, 164)               0         \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 1)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 808,781\n",
      "Trainable params: 808,781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[0.39984   ]\n",
      " [0.6963459 ]\n",
      " [1.0186565 ]\n",
      " [0.05746192]\n",
      " [0.6509048 ]\n",
      " [0.44645584]\n",
      " [0.13244222]\n",
      " [0.15760668]\n",
      " [0.2624913 ]\n",
      " [1.0126419 ]\n",
      " [0.41807622]\n",
      " [0.853155  ]\n",
      " [0.52132624]\n",
      " [0.587134  ]\n",
      " [0.53200907]\n",
      " [0.67887354]\n",
      " [0.40257764]\n",
      " [0.34441772]\n",
      " [0.38050863]\n",
      " [0.5203261 ]\n",
      " [1.0189228 ]\n",
      " [0.5416822 ]\n",
      " [0.26772976]\n",
      " [0.5242171 ]\n",
      " [0.35251865]\n",
      " [0.44001418]\n",
      " [0.52847624]\n",
      " [0.8407554 ]\n",
      " [0.51736486]\n",
      " [1.0091558 ]\n",
      " [0.19814509]\n",
      " [0.47485882]\n",
      " [0.36243737]\n",
      " [0.41607335]\n",
      " [0.34917852]\n",
      " [0.30917734]\n",
      " [0.49029738]\n",
      " [0.41042486]\n",
      " [1.00836   ]\n",
      " [0.47205698]\n",
      " [1.0077101 ]\n",
      " [1.0200061 ]\n",
      " [0.34424493]\n",
      " [0.5189649 ]\n",
      " [0.43873692]\n",
      " [0.22970775]\n",
      " [0.3891232 ]\n",
      " [0.33503154]\n",
      " [1.0163333 ]\n",
      " [0.4756722 ]\n",
      " [0.6730545 ]\n",
      " [0.34608874]\n",
      " [0.5450254 ]\n",
      " [0.3504541 ]\n",
      " [0.6659887 ]\n",
      " [0.7462544 ]\n",
      " [0.2969025 ]\n",
      " [0.54342926]\n",
      " [0.5492123 ]\n",
      " [0.57192934]\n",
      " [0.4639378 ]\n",
      " [0.54314685]\n",
      " [0.4616075 ]\n",
      " [1.0151308 ]\n",
      " [0.17386447]\n",
      " [0.35562193]\n",
      " [0.9638197 ]\n",
      " [1.0146362 ]\n",
      " [0.42387977]\n",
      " [0.5652581 ]\n",
      " [0.40497124]\n",
      " [1.0138879 ]\n",
      " [1.0110779 ]\n",
      " [0.5990205 ]\n",
      " [0.27983552]\n",
      " [0.31082684]\n",
      " [0.54682827]\n",
      " [0.50753695]\n",
      " [0.553063  ]\n",
      " [1.0180823 ]\n",
      " [0.18252334]\n",
      " [0.34341407]\n",
      " [0.5477354 ]\n",
      " [0.4860245 ]\n",
      " [0.38350806]\n",
      " [1.0162157 ]\n",
      " [1.0082802 ]\n",
      " [0.5778924 ]\n",
      " [0.34635442]\n",
      " [0.21242048]\n",
      " [0.3911388 ]\n",
      " [0.44456452]\n",
      " [0.26860622]\n",
      " [0.20829616]\n",
      " [0.3461768 ]\n",
      " [0.44662192]\n",
      " [0.30506164]\n",
      " [0.56903315]\n",
      " [0.34384453]\n",
      " [0.5954452 ]\n",
      " [0.6005737 ]\n",
      " [1.0178795 ]\n",
      " [0.46366224]\n",
      " [0.4074614 ]\n",
      " [0.4538876 ]\n",
      " [0.55483377]\n",
      " [0.50116885]\n",
      " [1.0150075 ]\n",
      " [0.46065676]\n",
      " [0.41194078]\n",
      " [1.0154972 ]\n",
      " [0.2261465 ]\n",
      " [0.49418157]\n",
      " [1.0196949 ]\n",
      " [1.0159788 ]\n",
      " [0.5577893 ]\n",
      " [1.0117877 ]\n",
      " [0.28224748]\n",
      " [0.12771367]\n",
      " [0.73381144]\n",
      " [0.3471317 ]\n",
      " [0.55449146]\n",
      " [0.27801087]\n",
      " [0.22609186]\n",
      " [1.0114295 ]\n",
      " [0.45365763]\n",
      " [0.3446964 ]\n",
      " [0.14200117]\n",
      " [0.38334632]\n",
      " [0.3589929 ]\n",
      " [0.30199304]\n",
      " [0.5534278 ]\n",
      " [0.15902202]\n",
      " [1.0141379 ]\n",
      " [0.636263  ]\n",
      " [0.32589227]\n",
      " [0.35715252]\n",
      " [1.0115482 ]\n",
      " [0.501984  ]\n",
      " [1.0113115 ]\n",
      " [0.60142255]\n",
      " [0.24750018]\n",
      " [0.43944323]\n",
      " [1.0137628 ]]\n",
      "\n",
      "Num of neurons1:  316 \n",
      "Num of neurons2 239 \n",
      "Epoch: 53 \n",
      "Batch size: 647 \n",
      "Learning rate: 0.011847341192321238\n",
      "\n",
      "Num of neurons1:  164 \n",
      "Num of neurons2 415 \n",
      "Epoch: 100 \n",
      "Batch size: 893 \n",
      "Learning rate: 0.003085245102166989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cu_dnnlstm_112 (CuDNNLSTM)  (None, 1, 164)            109552    \n",
      "                                                                 \n",
      " dropout_112 (Dropout)       (None, 1, 164)            0         \n",
      "                                                                 \n",
      " cu_dnnlstm_113 (CuDNNLSTM)  (None, 415)               964460    \n",
      "                                                                 \n",
      " dropout_113 (Dropout)       (None, 415)               0         \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 1)                 416       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,074,428\n",
      "Trainable params: 1,074,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[0.48991185]\n",
      " [0.5543677 ]\n",
      " [0.8192295 ]\n",
      " [0.54273075]\n",
      " [0.28784543]\n",
      " [0.2836841 ]\n",
      " [0.37032172]\n",
      " [0.49378026]\n",
      " [0.2908955 ]\n",
      " [0.34851262]\n",
      " [0.26416624]\n",
      " [0.5461944 ]\n",
      " [0.43541002]\n",
      " [0.5770466 ]\n",
      " [1.0041797 ]\n",
      " [0.48632285]\n",
      " [0.37172675]\n",
      " [0.36058813]\n",
      " [0.5096153 ]\n",
      " [0.9992413 ]\n",
      " [0.41363454]\n",
      " [0.59893894]\n",
      " [0.35905042]\n",
      " [0.5970737 ]\n",
      " [0.7285288 ]\n",
      " [0.55134624]\n",
      " [0.25010404]\n",
      " [0.24149074]\n",
      " [0.5680551 ]\n",
      " [0.39192396]\n",
      " [0.453631  ]\n",
      " [0.42846376]\n",
      " [0.50871027]\n",
      " [0.5079052 ]\n",
      " [0.3693417 ]\n",
      " [0.5094658 ]\n",
      " [0.9943148 ]\n",
      " [0.9960441 ]\n",
      " [0.5342617 ]\n",
      " [0.2887594 ]\n",
      " [0.33661824]\n",
      " [0.24086452]\n",
      " [0.3600385 ]\n",
      " [1.0028846 ]\n",
      " [0.47217226]\n",
      " [0.56101006]\n",
      " [0.5593679 ]\n",
      " [0.5796237 ]\n",
      " [0.06655904]\n",
      " [0.44975293]\n",
      " [1.0044641 ]\n",
      " [0.9933614 ]\n",
      " [0.48692054]\n",
      " [0.56350887]\n",
      " [1.0038676 ]\n",
      " [0.6093911 ]\n",
      " [0.67497295]\n",
      " [0.9964752 ]\n",
      " [1.00064   ]\n",
      " [0.36166784]\n",
      " [0.29733464]\n",
      " [0.3656093 ]\n",
      " [0.3536835 ]\n",
      " [0.45516497]\n",
      " [1.0036167 ]\n",
      " [0.35396263]\n",
      " [0.5436856 ]\n",
      " [0.5356175 ]\n",
      " [0.35014352]\n",
      " [0.42551774]\n",
      " [0.99414957]\n",
      " [0.5328974 ]\n",
      " [0.4811586 ]\n",
      " [0.27304927]\n",
      " [0.3203558 ]\n",
      " [0.5594972 ]\n",
      " [0.9959384 ]\n",
      " [0.38560176]\n",
      " [0.99669546]\n",
      " [0.50978684]\n",
      " [0.3583141 ]\n",
      " [0.52360487]\n",
      " [0.3977116 ]\n",
      " [0.55485624]\n",
      " [0.9955258 ]\n",
      " [0.53071463]\n",
      " [0.9976017 ]\n",
      " [0.35735703]\n",
      " [0.44728538]\n",
      " [0.603631  ]\n",
      " [0.5358435 ]\n",
      " [0.53651905]\n",
      " [0.59471774]\n",
      " [0.45529425]\n",
      " [0.12727332]\n",
      " [0.5523351 ]\n",
      " [0.5572699 ]\n",
      " [1.0005254 ]\n",
      " [0.99513036]\n",
      " [0.58145416]\n",
      " [0.16837734]\n",
      " [0.50153506]\n",
      " [0.3883693 ]\n",
      " [0.35884997]\n",
      " [0.29329568]\n",
      " [0.36563787]\n",
      " [0.49470598]\n",
      " [0.6354391 ]\n",
      " [0.9949397 ]\n",
      " [0.8470463 ]\n",
      " [0.48853627]\n",
      " [0.45344305]\n",
      " [0.40074775]\n",
      " [1.0042534 ]\n",
      " [0.556232  ]\n",
      " [0.8372919 ]\n",
      " [0.35447392]\n",
      " [1.0020751 ]\n",
      " [0.37535492]\n",
      " [0.21940501]\n",
      " [0.41226393]\n",
      " [0.5661646 ]\n",
      " [0.46234015]\n",
      " [0.46964565]\n",
      " [1.00218   ]\n",
      " [0.5682847 ]\n",
      " [0.5461952 ]\n",
      " [0.4625651 ]\n",
      " [0.3860625 ]\n",
      " [0.9943998 ]\n",
      " [0.4789434 ]\n",
      " [0.39818367]\n",
      " [0.99714416]\n",
      " [0.46732733]\n",
      " [1.0017551 ]\n",
      " [1.0019695 ]\n",
      " [0.45762113]\n",
      " [0.9948463 ]\n",
      " [0.99532574]\n",
      " [0.5874103 ]\n",
      " [0.4417032 ]\n",
      " [0.5188391 ]\n",
      " [0.3717016 ]\n",
      " [0.42244262]]\n",
      "\n",
      "Num of neurons1:  190 \n",
      "Num of neurons2 278 \n",
      "Epoch: 112 \n",
      "Batch size: 940 \n",
      "Learning rate: 0.011600521584147878\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cu_dnnlstm_114 (CuDNNLSTM)  (None, 1, 190)            146680    \n",
      "                                                                 \n",
      " dropout_114 (Dropout)       (None, 1, 190)            0         \n",
      "                                                                 \n",
      " cu_dnnlstm_115 (CuDNNLSTM)  (None, 278)               522640    \n",
      "                                                                 \n",
      " dropout_115 (Dropout)       (None, 278)               0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 1)                 279       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,599\n",
      "Trainable params: 669,599\n",
      "Non-trainable params: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "[[1.0124466 ]\n",
      " [0.5511161 ]\n",
      " [1.0074888 ]\n",
      " [0.35556197]\n",
      " [0.3414431 ]\n",
      " [1.0037646 ]\n",
      " [0.4473672 ]\n",
      " [0.5514795 ]\n",
      " [0.31413695]\n",
      " [1.0130938 ]\n",
      " [0.40832222]\n",
      " [0.29832226]\n",
      " [0.43963027]\n",
      " [0.2063686 ]\n",
      " [0.40347645]\n",
      " [0.3118952 ]\n",
      " [0.6607071 ]\n",
      " [0.79405487]\n",
      " [0.38030824]\n",
      " [0.700875  ]\n",
      " [0.26125544]\n",
      " [0.70811194]\n",
      " [1.0034658 ]\n",
      " [0.5962377 ]\n",
      " [0.3874299 ]\n",
      " [0.4266317 ]\n",
      " [0.47648048]\n",
      " [0.47317117]\n",
      " [0.45621264]\n",
      " [0.3501678 ]\n",
      " [0.44479516]\n",
      " [1.0060893 ]\n",
      " [0.5462098 ]\n",
      " [1.0077311 ]\n",
      " [0.37969282]\n",
      " [0.49241695]\n",
      " [0.27815   ]\n",
      " [1.0039252 ]\n",
      " [0.41597307]\n",
      " [0.40090954]\n",
      " [0.34476003]\n",
      " [0.35087177]\n",
      " [1.0095835 ]\n",
      " [0.5092871 ]\n",
      " [1.0131977 ]\n",
      " [0.2906507 ]\n",
      " [1.0082204 ]\n",
      " [0.3509268 ]\n",
      " [0.3444918 ]\n",
      " [1.003687  ]\n",
      " [0.72612333]\n",
      " [0.4832313 ]\n",
      " [0.37195903]\n",
      " [0.46606636]\n",
      " [0.15540393]\n",
      " [1.0059781 ]\n",
      " [1.009211  ]\n",
      " [1.0143366 ]\n",
      " [0.3457604 ]\n",
      " [0.4825545 ]\n",
      " [0.6134061 ]\n",
      " [0.40070212]\n",
      " [0.3488614 ]\n",
      " [0.26689318]\n",
      " [0.3817536 ]\n",
      " [1.0117643 ]\n",
      " [0.5625227 ]\n",
      " [0.39857903]\n",
      " [1.0094595 ]\n",
      " [0.7209432 ]\n",
      " [0.28463644]\n",
      " [0.55288047]\n",
      " [0.85515684]\n",
      " [0.40741315]\n",
      " [0.18665338]\n",
      " [0.4140211 ]\n",
      " [0.22849768]\n",
      " [1.0147434 ]\n",
      " [0.3242824 ]\n",
      " [0.5533923 ]\n",
      " [0.61878204]\n",
      " [0.5992873 ]\n",
      " [0.5029167 ]\n",
      " [0.32135803]\n",
      " [0.52788734]\n",
      " [0.4730469 ]\n",
      " [1.0040926 ]\n",
      " [1.0046345 ]\n",
      " [0.67058676]\n",
      " [0.52898204]\n",
      " [0.57049644]\n",
      " [0.35982004]\n",
      " [0.5269625 ]\n",
      " [0.849651  ]\n",
      " [1.0125573 ]\n",
      " [0.60283256]\n",
      " [0.51848733]\n",
      " [0.5846503 ]\n",
      " [0.56413275]\n",
      " [0.4241012 ]\n",
      " [0.38618514]\n",
      " [0.5495112 ]\n",
      " [0.25126126]\n",
      " [0.820583  ]\n",
      " [0.504795  ]\n",
      " [0.34181812]\n",
      " [0.5415158 ]\n",
      " [1.004008  ]\n",
      " [0.15862755]\n",
      " [0.36556688]\n",
      " [1.0054371 ]\n",
      " [0.38302988]\n",
      " [0.27698636]\n",
      " [1.0151697 ]\n",
      " [0.34677446]\n",
      " [0.4795894 ]\n",
      " [0.3669389 ]\n",
      " [1.0088385 ]\n",
      " [0.53791344]\n",
      " [0.5155351 ]\n",
      " [0.36775666]\n",
      " [0.490734  ]\n",
      " [0.462244  ]\n",
      " [0.34535468]\n",
      " [0.33552092]\n",
      " [1.0041789 ]\n",
      " [0.2667702 ]\n",
      " [0.17733623]\n",
      " [0.433651  ]\n",
      " [0.26360682]\n",
      " [1.0116476 ]\n",
      " [0.4432247 ]\n",
      " [0.9595986 ]\n",
      " [0.35227132]\n",
      " [1.0103247 ]\n",
      " [0.2699405 ]\n",
      " [1.0138879 ]\n",
      " [0.2747316 ]\n",
      " [0.6763764 ]\n",
      " [0.56027555]\n",
      " [0.5223664 ]\n",
      " [1.0127752 ]\n",
      " [0.34526289]\n",
      " [0.37288016]]\n",
      "\n",
      "Num of neurons1:  343 \n",
      "Num of neurons2 164 \n",
      "Epoch: 116 \n",
      "Batch size: 940 \n",
      "Learning rate: 0.011600521584147878\n",
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cu_dnnlstm_116 (CuDNNLSTM)  (None, 1, 343)            474712    \n",
      "                                                                 \n",
      " dropout_116 (Dropout)       (None, 1, 343)            0         \n",
      "                                                                 \n",
      " cu_dnnlstm_117 (CuDNNLSTM)  (None, 164)               333904    \n",
      "                                                                 \n",
      " dropout_117 (Dropout)       (None, 164)               0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 1)                 165       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 808,781\n",
      "Trainable params: 808,781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[0.552807  ]\n",
      " [0.45875204]\n",
      " [0.34045014]\n",
      " [0.3888515 ]\n",
      " [1.0106084 ]\n",
      " [1.005957  ]\n",
      " [0.687026  ]\n",
      " [0.39545056]\n",
      " [0.22815698]\n",
      " [0.30861154]\n",
      " [0.416913  ]\n",
      " [0.39097023]\n",
      " [1.0122977 ]\n",
      " [0.45152056]\n",
      " [0.4128835 ]\n",
      " [0.67604715]\n",
      " [0.5409121 ]\n",
      " [0.39775145]\n",
      " [1.0146105 ]\n",
      " [0.60523665]\n",
      " [0.21164587]\n",
      " [0.45113373]\n",
      " [0.4818565 ]\n",
      " [0.81270623]\n",
      " [0.5474584 ]\n",
      " [0.45129088]\n",
      " [0.43766657]\n",
      " [0.35317248]\n",
      " [0.83955705]\n",
      " [0.92527586]\n",
      " [0.53916836]\n",
      " [0.3378965 ]\n",
      " [0.52329415]\n",
      " [0.70060724]\n",
      " [0.60366946]\n",
      " [0.5519528 ]\n",
      " [0.3424548 ]\n",
      " [0.38549617]\n",
      " [0.41575718]\n",
      " [0.4066409 ]\n",
      " [0.56896454]\n",
      " [0.36883974]\n",
      " [0.5717768 ]\n",
      " [0.34732515]\n",
      " [0.35296088]\n",
      " [0.40962932]\n",
      " [0.4878744 ]\n",
      " [0.5443045 ]\n",
      " [0.3447514 ]\n",
      " [0.34117362]\n",
      " [0.4613745 ]\n",
      " [0.0712567 ]\n",
      " [1.0130924 ]\n",
      " [0.5265701 ]\n",
      " [0.34488797]\n",
      " [0.46502113]\n",
      " [0.6852603 ]\n",
      " [1.0152053 ]\n",
      " [0.54250515]\n",
      " [0.5439441 ]\n",
      " [0.46747983]\n",
      " [0.4000439 ]\n",
      " [1.0070661 ]\n",
      " [0.47313705]\n",
      " [0.40811518]\n",
      " [0.57350755]\n",
      " [0.39252332]\n",
      " [0.64815044]\n",
      " [0.58087456]\n",
      " [0.40277967]\n",
      " [0.2898454 ]\n",
      " [0.3548997 ]\n",
      " [1.0102367 ]\n",
      " [0.4917109 ]\n",
      " [0.49950233]\n",
      " [0.28867605]\n",
      " [0.8553038 ]\n",
      " [0.5928211 ]\n",
      " [0.62927127]\n",
      " [0.24531825]\n",
      " [1.0158963 ]\n",
      " [1.0081344 ]\n",
      " [0.7793675 ]\n",
      " [0.45638376]\n",
      " [0.4439674 ]\n",
      " [0.26945344]\n",
      " [0.5120722 ]\n",
      " [0.44855127]\n",
      " [0.6129329 ]\n",
      " [1.0055444 ]\n",
      " [1.0136302 ]\n",
      " [1.0156442 ]\n",
      " [0.3865764 ]\n",
      " [0.45646057]\n",
      " [0.3975428 ]\n",
      " [0.42246336]\n",
      " [1.0037135 ]\n",
      " [0.54063034]\n",
      " [0.8192226 ]\n",
      " [0.5489197 ]\n",
      " [0.5963884 ]\n",
      " [1.0127573 ]\n",
      " [0.6133855 ]\n",
      " [0.3810464 ]\n",
      " [1.0144259 ]\n",
      " [1.0093658 ]\n",
      " [0.5435223 ]\n",
      " [0.26629284]\n",
      " [0.2607955 ]\n",
      " [0.723092  ]\n",
      " [1.0108553 ]\n",
      " [0.55002785]\n",
      " [0.323666  ]\n",
      " [1.0066108 ]\n",
      " [0.40027735]\n",
      " [0.35027018]\n",
      " [0.33567482]\n",
      " [0.34855065]\n",
      " [0.15228531]\n",
      " [0.3063751 ]\n",
      " [0.4998314 ]\n",
      " [0.30183116]\n",
      " [0.43639097]\n",
      " [1.0077727 ]\n",
      " [0.5504531 ]\n",
      " [0.40463126]\n",
      " [0.42985952]\n",
      " [0.43295038]\n",
      " [0.2997836 ]\n",
      " [0.5491142 ]\n",
      " [0.32774726]\n",
      " [0.4798851 ]\n",
      " [0.5161912 ]\n",
      " [0.44778252]\n",
      " [0.3986425 ]\n",
      " [0.36687666]\n",
      " [0.50835025]\n",
      " [0.11835876]\n",
      " [0.53435194]\n",
      " [0.40191993]\n",
      " [0.3562784 ]\n",
      " [0.36282727]\n",
      " [1.0061706 ]\n",
      " [0.40228677]]\n",
      "\n",
      "Num of neurons1:  190 \n",
      "Num of neurons2 278 \n",
      "Epoch: 112 \n",
      "Batch size: 921 \n",
      "Learning rate: 0.012217570604581276\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cu_dnnlstm_118 (CuDNNLSTM)  (None, 1, 190)            146680    \n",
      "                                                                 \n",
      " dropout_118 (Dropout)       (None, 1, 190)            0         \n",
      "                                                                 \n",
      " cu_dnnlstm_119 (CuDNNLSTM)  (None, 278)               522640    \n",
      "                                                                 \n",
      " dropout_119 (Dropout)       (None, 278)               0         \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 1)                 279       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,599\n",
      "Trainable params: 669,599\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.27724355]\n",
      " [0.29287645]\n",
      " [0.63245577]\n",
      " [0.46459597]\n",
      " [1.0146806 ]\n",
      " [0.33868685]\n",
      " [0.44578886]\n",
      " [1.004724  ]\n",
      " [0.37021115]\n",
      " [1.0105789 ]\n",
      " [0.5487481 ]\n",
      " [0.733031  ]\n",
      " [1.0145175 ]\n",
      " [0.45964688]\n",
      " [0.35127088]\n",
      " [0.51136714]\n",
      " [0.49331096]\n",
      " [0.24776152]\n",
      " [0.49290016]\n",
      " [0.3553416 ]\n",
      " [0.13746154]\n",
      " [0.41803482]\n",
      " [0.3098692 ]\n",
      " [0.30924153]\n",
      " [0.18837614]\n",
      " [0.4391874 ]\n",
      " [0.57241714]\n",
      " [1.0038033 ]\n",
      " [0.4646368 ]\n",
      " [0.34574834]\n",
      " [0.4544345 ]\n",
      " [0.36198395]\n",
      " [0.54722255]\n",
      " [0.45029685]\n",
      " [0.4827278 ]\n",
      " [0.5630307 ]\n",
      " [0.3687696 ]\n",
      " [1.0068245 ]\n",
      " [0.40321815]\n",
      " [0.48763075]\n",
      " [0.40325585]\n",
      " [0.36939156]\n",
      " [0.63633585]\n",
      " [0.56578165]\n",
      " [0.87028503]\n",
      " [0.6200209 ]\n",
      " [0.20820428]\n",
      " [0.50358415]\n",
      " [0.48255444]\n",
      " [0.35038525]\n",
      " [0.49633488]\n",
      " [0.3116571 ]\n",
      " [0.40885872]\n",
      " [0.27688867]\n",
      " [0.51369214]\n",
      " [0.42707512]\n",
      " [0.2261706 ]\n",
      " [0.47471726]\n",
      " [0.3608291 ]\n",
      " [0.56085426]\n",
      " [0.24510145]\n",
      " [0.5456484 ]\n",
      " [0.34473646]\n",
      " [0.6689169 ]\n",
      " [0.41851044]\n",
      " [0.51295865]\n",
      " [1.0058242 ]\n",
      " [1.0037302 ]\n",
      " [1.0062596 ]\n",
      " [0.35414055]\n",
      " [0.5386983 ]\n",
      " [1.0144334 ]\n",
      " [0.47571844]\n",
      " [0.35094336]\n",
      " [1.0035225 ]\n",
      " [0.5794017 ]\n",
      " [0.96016306]\n",
      " [0.41084942]\n",
      " [0.6320835 ]\n",
      " [0.34917858]\n",
      " [0.52673286]\n",
      " [1.0143477 ]\n",
      " [0.30057615]\n",
      " [0.5291705 ]\n",
      " [0.4765301 ]\n",
      " [0.32962802]\n",
      " [0.35081708]\n",
      " [0.29143697]\n",
      " [1.0041965 ]\n",
      " [0.34504256]\n",
      " [1.0091072 ]\n",
      " [1.0080072 ]\n",
      " [0.3796324 ]\n",
      " [0.48132125]\n",
      " [0.34999162]\n",
      " [0.41193974]\n",
      " [0.3995316 ]\n",
      " [0.4421917 ]\n",
      " [1.0115352 ]\n",
      " [0.40194312]\n",
      " [0.5361787 ]\n",
      " [0.4949935 ]\n",
      " [0.3700055 ]\n",
      " [0.3056968 ]\n",
      " [0.50277275]\n",
      " [0.59014034]\n",
      " [0.46201482]\n",
      " [0.5315384 ]\n",
      " [0.3564536 ]\n",
      " [0.41893074]\n",
      " [0.26229256]\n",
      " [0.46864933]\n",
      " [1.0122261 ]\n",
      " [0.39829195]\n",
      " [0.3570422 ]\n",
      " [0.1264429 ]\n",
      " [0.42991692]\n",
      " [1.0081284 ]\n",
      " [0.35937184]\n",
      " [0.33198   ]\n",
      " [0.40488547]\n",
      " [0.34333986]\n",
      " [1.0153224 ]\n",
      " [1.0086163 ]\n",
      " [1.0039552 ]\n",
      " [1.015443  ]\n",
      " [0.42312834]\n",
      " [0.5083079 ]\n",
      " [0.42692405]\n",
      " [1.0077664 ]\n",
      " [0.4868582 ]\n",
      " [0.54924923]\n",
      " [0.45439652]\n",
      " [0.4763947 ]\n",
      " [1.0135068 ]\n",
      " [0.2765006 ]\n",
      " [0.6042893 ]\n",
      " [1.0109411 ]\n",
      " [0.85101545]\n",
      " [0.3604716 ]\n",
      " [0.3730089 ]\n",
      " [0.34420902]\n",
      " [0.46413767]\n",
      " [0.56735665]]\n",
      "\n",
      "Num of neurons1:  190 \n",
      "Num of neurons2 278 \n",
      "Epoch: 112 \n",
      "Batch size: 940 \n",
      "Learning rate: 0.011600521584147878\n",
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cu_dnnlstm_120 (CuDNNLSTM)  (None, 1, 190)            146680    \n",
      "                                                                 \n",
      " dropout_120 (Dropout)       (None, 1, 190)            0         \n",
      "                                                                 \n",
      " cu_dnnlstm_121 (CuDNNLSTM)  (None, 278)               522640    \n",
      "                                                                 \n",
      " dropout_121 (Dropout)       (None, 278)               0         \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 1)                 279       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,599\n",
      "Trainable params: 669,599\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5977578 ]\n",
      " [0.33804995]\n",
      " [0.73887336]\n",
      " [0.2778737 ]\n",
      " [0.5495578 ]\n",
      " [0.3885708 ]\n",
      " [0.42567438]\n",
      " [0.39931154]\n",
      " [0.34426168]\n",
      " [0.5042554 ]\n",
      " [1.0134888 ]\n",
      " [1.0114332 ]\n",
      " [0.4777586 ]\n",
      " [0.55727327]\n",
      " [1.0049919 ]\n",
      " [0.85691667]\n",
      " [0.5499964 ]\n",
      " [0.4892419 ]\n",
      " [0.31494978]\n",
      " [0.92670625]\n",
      " [0.42287913]\n",
      " [0.41513905]\n",
      " [0.3996538 ]\n",
      " [0.45830545]\n",
      " [0.26741686]\n",
      " [0.40205482]\n",
      " [0.4269858 ]\n",
      " [0.4957392 ]\n",
      " [0.37489423]\n",
      " [0.33277795]\n",
      " [0.41662005]\n",
      " [0.11810386]\n",
      " [1.0122948 ]\n",
      " [0.5283529 ]\n",
      " [0.407431  ]\n",
      " [0.5292797 ]\n",
      " [0.6663753 ]\n",
      " [0.5019798 ]\n",
      " [1.0127791 ]\n",
      " [0.3086463 ]\n",
      " [0.6056949 ]\n",
      " [0.3771837 ]\n",
      " [0.29498416]\n",
      " [0.48384696]\n",
      " [0.60726136]\n",
      " [0.5023446 ]\n",
      " [0.4818479 ]\n",
      " [0.5316281 ]\n",
      " [0.4383332 ]\n",
      " [0.46542385]\n",
      " [0.34567282]\n",
      " [0.5705935 ]\n",
      " [0.5392294 ]\n",
      " [1.0088569 ]\n",
      " [0.2788378 ]\n",
      " [0.33942917]\n",
      " [0.4231786 ]\n",
      " [0.3353387 ]\n",
      " [1.0121726 ]\n",
      " [0.43954736]\n",
      " [0.38133   ]\n",
      " [0.67341524]\n",
      " [0.52858526]\n",
      " [0.38830823]\n",
      " [0.48087487]\n",
      " [0.26424187]\n",
      " [0.50610757]\n",
      " [0.48452532]\n",
      " [0.18044192]\n",
      " [0.7450965 ]\n",
      " [0.4916336 ]\n",
      " [1.0050576 ]\n",
      " [0.29112023]\n",
      " [0.44893688]\n",
      " [0.49373206]\n",
      " [1.016466  ]\n",
      " [0.42339358]\n",
      " [1.0064588 ]\n",
      " [0.5375581 ]\n",
      " [0.53503084]\n",
      " [0.5445297 ]\n",
      " [0.34400865]\n",
      " [1.0136044 ]\n",
      " [0.5411913 ]\n",
      " [0.45320207]\n",
      " [1.0162274 ]\n",
      " [0.4609589 ]\n",
      " [1.0161442 ]\n",
      " [0.36387545]\n",
      " [0.23682153]\n",
      " [1.0068549 ]\n",
      " [0.2666583 ]\n",
      " [0.4099169 ]\n",
      " [0.4744413 ]\n",
      " [0.5525549 ]\n",
      " [0.33885252]\n",
      " [0.48635137]\n",
      " [0.40716234]\n",
      " [0.35320774]\n",
      " [0.47160918]\n",
      " [0.46067825]\n",
      " [0.5529191 ]\n",
      " [0.5364902 ]\n",
      " [0.27667907]\n",
      " [0.44511476]\n",
      " [0.5948501 ]\n",
      " [0.44689962]\n",
      " [0.46326473]\n",
      " [0.34924525]\n",
      " [1.0137194 ]\n",
      " [0.13982671]\n",
      " [1.0113093 ]\n",
      " [1.0079294 ]\n",
      " [0.5363721 ]\n",
      " [0.526241  ]\n",
      " [0.12519336]\n",
      " [0.17763126]\n",
      " [1.0078174 ]\n",
      " [1.0170771 ]\n",
      " [1.0170175 ]\n",
      " [0.5138079 ]\n",
      " [0.65444595]\n",
      " [0.445995  ]\n",
      " [0.5366489 ]\n",
      " [0.36446935]\n",
      " [0.6623226 ]\n",
      " [0.51771575]\n",
      " [0.4270994 ]\n",
      " [0.28994375]\n",
      " [0.5639832 ]\n",
      " [0.4504567 ]\n",
      " [0.37079424]\n",
      " [1.0110612 ]\n",
      " [0.45744136]\n",
      " [0.58526075]\n",
      " [0.57228446]\n",
      " [1.0059079 ]\n",
      " [1.0152248 ]\n",
      " [0.3101614 ]\n",
      " [0.34355542]\n",
      " [0.2518505 ]\n",
      " [0.553981  ]\n",
      " [0.67221475]\n",
      " [0.39721248]]\n",
      "\n",
      "Num of neurons1:  190 \n",
      "Num of neurons2 278 \n",
      "Epoch: 112 \n",
      "Batch size: 940 \n",
      "Learning rate: 0.011600521584147878\n",
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cu_dnnlstm_122 (CuDNNLSTM)  (None, 1, 190)            146680    \n",
      "                                                                 \n",
      " dropout_122 (Dropout)       (None, 1, 190)            0         \n",
      "                                                                 \n",
      " cu_dnnlstm_123 (CuDNNLSTM)  (None, 278)               522640    \n",
      "                                                                 \n",
      " dropout_123 (Dropout)       (None, 278)               0         \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 1)                 279       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,599\n",
      "Trainable params: 669,599\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4343589 ]\n",
      " [0.5151627 ]\n",
      " [0.5355396 ]\n",
      " [1.0015625 ]\n",
      " [0.38270268]\n",
      " [0.6635083 ]\n",
      " [1.0123706 ]\n",
      " [0.5636878 ]\n",
      " [0.30868986]\n",
      " [0.3990046 ]\n",
      " [0.334457  ]\n",
      " [0.47097927]\n",
      " [0.4744428 ]\n",
      " [0.4280879 ]\n",
      " [0.05481254]\n",
      " [1.0043544 ]\n",
      " [0.5228619 ]\n",
      " [0.3294848 ]\n",
      " [1.0107348 ]\n",
      " [0.4068222 ]\n",
      " [0.29507506]\n",
      " [0.34540328]\n",
      " [0.3496795 ]\n",
      " [0.6020657 ]\n",
      " [0.47376826]\n",
      " [0.5176412 ]\n",
      " [0.3352857 ]\n",
      " [0.14034587]\n",
      " [0.38381612]\n",
      " [0.6226205 ]\n",
      " [1.0027337 ]\n",
      " [0.46267402]\n",
      " [0.7932977 ]\n",
      " [0.27898273]\n",
      " [0.5356976 ]\n",
      " [1.0082318 ]\n",
      " [0.6140286 ]\n",
      " [0.6705057 ]\n",
      " [1.0045862 ]\n",
      " [0.30210993]\n",
      " [0.35174343]\n",
      " [0.5244135 ]\n",
      " [0.35885045]\n",
      " [1.0088314 ]\n",
      " [0.5140932 ]\n",
      " [0.4770995 ]\n",
      " [0.29169834]\n",
      " [0.55327904]\n",
      " [0.48698488]\n",
      " [0.4327459 ]\n",
      " [1.0052967 ]\n",
      " [0.54454964]\n",
      " [1.0111392 ]\n",
      " [0.50087124]\n",
      " [0.47379804]\n",
      " [0.4384964 ]\n",
      " [0.38466796]\n",
      " [0.63405913]\n",
      " [0.10834863]\n",
      " [1.0026349 ]\n",
      " [0.58794206]\n",
      " [0.40608534]\n",
      " [0.16557938]\n",
      " [1.007377  ]\n",
      " [0.56737745]\n",
      " [0.6030479 ]\n",
      " [0.22929806]\n",
      " [0.291486  ]\n",
      " [0.37060428]\n",
      " [0.52635926]\n",
      " [0.6788704 ]\n",
      " [0.6395993 ]\n",
      " [0.3221901 ]\n",
      " [0.53826463]\n",
      " [0.4016727 ]\n",
      " [0.41519877]\n",
      " [0.5102925 ]\n",
      " [0.31738162]\n",
      " [0.5340878 ]\n",
      " [0.5354222 ]\n",
      " [0.41016176]\n",
      " [1.0124458 ]\n",
      " [0.35821438]\n",
      " [0.70064396]\n",
      " [0.40542948]\n",
      " [0.45962867]\n",
      " [0.4999463 ]\n",
      " [1.0014068 ]\n",
      " [0.29122424]\n",
      " [0.54520077]\n",
      " [0.21269679]\n",
      " [0.47183362]\n",
      " [1.0074998 ]\n",
      " [0.67169887]\n",
      " [1.0120512 ]\n",
      " [0.48061922]\n",
      " [1.0057805 ]\n",
      " [0.38766342]\n",
      " [1.0127277 ]\n",
      " [0.4303133 ]\n",
      " [0.4110771 ]\n",
      " [0.40150315]\n",
      " [0.3462213 ]\n",
      " [0.5129509 ]\n",
      " [0.3127297 ]\n",
      " [0.4453522 ]\n",
      " [0.67055035]\n",
      " [0.52744406]\n",
      " [0.46262148]\n",
      " [0.5306055 ]\n",
      " [1.0066383 ]\n",
      " [0.5501146 ]\n",
      " [1.0087124 ]\n",
      " [0.3084113 ]\n",
      " [0.35484532]\n",
      " [0.17247894]\n",
      " [0.27683103]\n",
      " [0.38788003]\n",
      " [0.403128  ]\n",
      " [0.5129726 ]\n",
      " [0.48470664]\n",
      " [0.36516324]\n",
      " [0.12591952]\n",
      " [0.342266  ]\n",
      " [0.34264076]\n",
      " [0.38471955]\n",
      " [0.48116937]\n",
      " [0.5879864 ]\n",
      " [0.34541595]\n",
      " [0.3387863 ]\n",
      " [0.4491583 ]\n",
      " [1.0029356 ]\n",
      " [0.45687768]\n",
      " [1.0012585 ]\n",
      " [0.5935875 ]\n",
      " [0.34235457]\n",
      " [0.52171814]\n",
      " [0.56088436]\n",
      " [0.45076838]\n",
      " [1.0095314 ]\n",
      " [0.34562084]\n",
      " [0.5462293 ]\n",
      " [0.29608756]\n",
      " [0.2759105 ]]\n",
      "\n",
      "Num of neurons1:  88 \n",
      "Num of neurons2 114 \n",
      "Epoch: 122 \n",
      "Batch size: 693 \n",
      "Learning rate: 0.01123029217188784\n",
      "\n",
      "Num of neurons1:  446 \n",
      "Num of neurons2 292 \n",
      "Epoch: 116 \n",
      "Batch size: 921 \n",
      "Learning rate: 0.012217570604581276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cu_dnnlstm_124 (CuDNNLSTM)  (None, 1, 446)            801016    \n",
      "                                                                 \n",
      " dropout_124 (Dropout)       (None, 1, 446)            0         \n",
      "                                                                 \n",
      " cu_dnnlstm_125 (CuDNNLSTM)  (None, 292)               864320    \n",
      "                                                                 \n",
      " dropout_125 (Dropout)       (None, 292)               0         \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 1)                 293       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,665,629\n",
      "Trainable params: 1,665,629\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[1.0049168 ]\n",
      " [0.40672326]\n",
      " [0.08220419]\n",
      " [0.44424704]\n",
      " [0.4839584 ]\n",
      " [0.3378307 ]\n",
      " [1.0029905 ]\n",
      " [0.4027993 ]\n",
      " [0.4588171 ]\n",
      " [0.3399744 ]\n",
      " [0.33626845]\n",
      " [0.34799978]\n",
      " [0.67660546]\n",
      " [0.34948128]\n",
      " [0.5361871 ]\n",
      " [1.0109054 ]\n",
      " [0.56287235]\n",
      " [1.006058  ]\n",
      " [0.5463223 ]\n",
      " [0.61292475]\n",
      " [1.0078384 ]\n",
      " [0.45041806]\n",
      " [0.37072176]\n",
      " [1.0026432 ]\n",
      " [0.5195761 ]\n",
      " [0.9597039 ]\n",
      " [0.533067  ]\n",
      " [1.0085508 ]\n",
      " [0.5780665 ]\n",
      " [0.45961303]\n",
      " [0.57718813]\n",
      " [1.0113113 ]\n",
      " [0.57140845]\n",
      " [0.29385868]\n",
      " [0.38435003]\n",
      " [0.40682158]\n",
      " [0.3604949 ]\n",
      " [0.55861133]\n",
      " [1.0059414 ]\n",
      " [1.00448   ]\n",
      " [0.6247609 ]\n",
      " [0.6662398 ]\n",
      " [0.4082396 ]\n",
      " [0.3984135 ]\n",
      " [0.50998706]\n",
      " [0.54505223]\n",
      " [0.29859868]\n",
      " [1.0114098 ]\n",
      " [0.34774145]\n",
      " [0.31708032]\n",
      " [0.263005  ]\n",
      " [1.0101559 ]\n",
      " [0.351503  ]\n",
      " [0.47494414]\n",
      " [1.003752  ]\n",
      " [0.3131214 ]\n",
      " [0.47895998]\n",
      " [0.35656855]\n",
      " [0.4664115 ]\n",
      " [0.17046636]\n",
      " [0.42357752]\n",
      " [0.26921627]\n",
      " [0.5850111 ]\n",
      " [0.4154183 ]\n",
      " [0.41479602]\n",
      " [0.5423095 ]\n",
      " [0.52954394]\n",
      " [0.55215585]\n",
      " [0.6703674 ]\n",
      " [0.17629412]\n",
      " [0.37969944]\n",
      " [1.0040578 ]\n",
      " [0.36339298]\n",
      " [0.4869021 ]\n",
      " [0.16326357]\n",
      " [0.34247583]\n",
      " [0.36457095]\n",
      " [0.24469821]\n",
      " [0.45427856]\n",
      " [0.56778604]\n",
      " [0.5252247 ]\n",
      " [0.31125194]\n",
      " [0.494898  ]\n",
      " [0.39684096]\n",
      " [0.45147514]\n",
      " [0.4154037 ]\n",
      " [0.4579655 ]\n",
      " [0.36689046]\n",
      " [0.47091386]\n",
      " [1.0130095 ]\n",
      " [0.34916815]\n",
      " [0.3112822 ]\n",
      " [0.68564576]\n",
      " [0.35850936]\n",
      " [0.3729586 ]\n",
      " [0.40857327]\n",
      " [0.48065338]\n",
      " [0.34995526]\n",
      " [0.4948799 ]\n",
      " [0.49903372]\n",
      " [0.12156609]\n",
      " [0.15334836]\n",
      " [0.56979805]\n",
      " [0.34613395]\n",
      " [0.36148643]\n",
      " [0.49937144]\n",
      " [0.6065094 ]\n",
      " [0.18780446]\n",
      " [1.0031732 ]\n",
      " [0.34614658]\n",
      " [0.5073661 ]\n",
      " [0.4122753 ]\n",
      " [0.34925836]\n",
      " [0.44479206]\n",
      " [0.21807076]\n",
      " [0.46466494]\n",
      " [1.0042669 ]\n",
      " [1.0127962 ]\n",
      " [0.35330483]\n",
      " [0.64654917]\n",
      " [0.15555827]\n",
      " [0.40065   ]\n",
      " [0.5256466 ]\n",
      " [0.27743945]\n",
      " [0.5195541 ]\n",
      " [0.42446724]\n",
      " [0.4800009 ]\n",
      " [0.43726414]\n",
      " [0.34568515]\n",
      " [0.39286277]\n",
      " [1.0071224 ]\n",
      " [0.30384883]\n",
      " [0.43405694]\n",
      " [0.3329338 ]\n",
      " [1.0061749 ]\n",
      " [0.46286294]\n",
      " [0.438288  ]\n",
      " [1.0039549 ]\n",
      " [0.39518234]\n",
      " [1.0025603 ]\n",
      " [0.38255575]\n",
      " [0.3815518 ]\n",
      " [1.0128692 ]\n",
      " [0.6058749 ]]\n",
      "\n",
      "Num of neurons1:  343 \n",
      "Num of neurons2 164 \n",
      "Epoch: 116 \n",
      "Batch size: 920 \n",
      "Learning rate: 0.011723931388234559\n",
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cu_dnnlstm_126 (CuDNNLSTM)  (None, 1, 343)            474712    \n",
      "                                                                 \n",
      " dropout_126 (Dropout)       (None, 1, 343)            0         \n",
      "                                                                 \n",
      " cu_dnnlstm_127 (CuDNNLSTM)  (None, 164)               333904    \n",
      "                                                                 \n",
      " dropout_127 (Dropout)       (None, 164)               0         \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 1)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 808,781\n",
      "Trainable params: 808,781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart time:\u001b[39m\u001b[38;5;124m\"\u001b[39m, start_time)\n\u001b[1;32m---> 24\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meaSimple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoolbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcxpb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutpb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngen\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_generations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnd time:\u001b[39m\u001b[38;5;124m'\u001b[39m,end_time)\n",
      "File \u001b[1;32mD:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\deap\\algorithms.py:173\u001b[0m, in \u001b[0;36meaSimple\u001b[1;34m(population, toolbox, cxpb, mutpb, ngen, stats, halloffame, verbose)\u001b[0m\n\u001b[0;32m    171\u001b[0m invalid_ind \u001b[38;5;241m=\u001b[39m [ind \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m offspring \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ind\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalid]\n\u001b[0;32m    172\u001b[0m fitnesses \u001b[38;5;241m=\u001b[39m toolbox\u001b[38;5;241m.\u001b[39mmap(toolbox\u001b[38;5;241m.\u001b[39mevaluate, invalid_ind)\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, fit \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(invalid_ind, fitnesses):\n\u001b[0;32m    174\u001b[0m     ind\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m fit\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Update the hall of fame with the generated individuals\u001b[39;00m\n",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36mtrain_evaluate\u001b[1;34m(ga_individual_solution)\u001b[0m\n\u001b[0;32m     48\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m,optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m---> 51\u001b[0m history\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_modified\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test_modified)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_pred)\n",
      "File \u001b[1;32mD:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mD:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mD:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:980\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    977\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;66;03m# stateless function.\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m   _, _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    983\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    984\u001b[0m           \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mD:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2955\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2952\u001b[0m \u001b[38;5;124;03m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m-> 2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[0;32m   2957\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mgraph_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[1;32mD:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3292\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3288\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[0;32m   3289\u001b[0m       args, kwargs, flat_args, filtered_flat_args)\n\u001b[0;32m   3291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39madd_call_context(cache_key\u001b[38;5;241m.\u001b[39mcall_context)\n\u001b[1;32m-> 3292\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39madd(cache_key, cache_key_deletion_observer,\n\u001b[0;32m   3294\u001b[0m                          graph_function)\n\u001b[0;32m   3296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[1;32mD:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3130\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3125\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3126\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   3127\u001b[0m ]\n\u001b[0;32m   3128\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[0;32m   3129\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 3130\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3133\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3135\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3138\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3139\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   3141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[0;32m   3142\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[0;32m   3143\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   3144\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   3145\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   3146\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   3147\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   3148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mD:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1053\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1051\u001b[0m   deps_control_manager \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mNullContextmanager()\n\u001b[1;32m-> 1053\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m func_graph\u001b[38;5;241m.\u001b[39mas_default(), deps_control_manager \u001b[38;5;28;01mas\u001b[39;00m deps_ctx:\n\u001b[0;32m   1054\u001b[0m   current_scope \u001b[38;5;241m=\u001b[39m variable_scope\u001b[38;5;241m.\u001b[39mget_variable_scope()\n\u001b[0;32m   1055\u001b[0m   default_use_resource \u001b[38;5;241m=\u001b[39m current_scope\u001b[38;5;241m.\u001b[39muse_resource\n",
      "File \u001b[1;32mD:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py:420\u001b[0m, in \u001b[0;36mAutomaticControlDependencies.__exit__\u001b[1;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[0;32m    417\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    418\u001b[0m control_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m \u001b[38;5;129;01min\u001b[39;00m MUST_RUN_ORDER_INSENSITIVE_STATEFUL_OPS:\n\u001b[0;32m    421\u001b[0m   \u001b[38;5;66;03m# This will add it to self._independent_ops, but also mark it with an\u001b[39;00m\n\u001b[0;32m    422\u001b[0m   \u001b[38;5;66;03m# attribute.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_independently(op)\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_independent_ops:\n",
      "File \u001b[1;32mD:\\Users\\Admin\\anaconda3\\envs\\Experment\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2583\u001b[0m, in \u001b[0;36mOperation.type\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   2581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   2582\u001b[0m   \u001b[38;5;124;03m\"\"\"The type of the op (e.g. `\"MatMul\"`).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2583\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_OperationOpType\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c_op\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "population_size = 50\n",
    "num_generations = 10\n",
    "gene_length = 42\n",
    "\n",
    "''' 以下是使用deap创造一个遗产算法实例 '''\n",
    "# As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "# In case, when you want to maximize accuracy for instance, use 1.0\n",
    "creator.create('FitnessMin', base.Fitness, weights = (-1.0,))\n",
    "creator.create('Individual', list , fitness = creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "toolbox.register('mate', tools.cxOrdered)\n",
    "toolbox.register('mutate', tools.mutShuffleIndexes, indpb = 0.6)\n",
    "toolbox.register('select', tools.selRoulette)\n",
    "toolbox.register('evaluate', train_evaluate)\n",
    "\n",
    "population = toolbox.population(n = population_size)\n",
    "start_time = datetime.now()\n",
    "print(\"Start time:\", start_time)\n",
    "r = algorithms.eaSimple(population, toolbox, cxpb = 0.4, mutpb = 0.1, ngen = num_generations, verbose = False)\n",
    "end_time = datetime.now()\n",
    "print('End time:',end_time)\n",
    "time_elapsed = end_time-start_time\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08792638",
   "metadata": {},
   "source": [
    "记录这个寻找的最优的个体，并记录参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a89b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consolidate the best individuals\n",
    "best_individuals = tools.selBest(population,k = 1)\n",
    "best_num_neurons1 = None\n",
    "best_num_neurons2 = None\n",
    "best_epoch = None\n",
    "best_batch_size = None\n",
    "best_learning_rate = None\n",
    "\n",
    "for bi in best_individuals:\n",
    "    num_neurons1_bits = BitArray(bi[0:9])\n",
    "    num_neurons2_bits = BitArray(bi[9:18])\n",
    "    epoch_bits = BitArray(bi[18:25])\n",
    "    batch_size_bits = BitArray(bi[25:35])\n",
    "    learning_rate_bits = BitArray(bi[35:]) \n",
    "    \n",
    "    best_num_neurons1 = num_neurons1_bits.uint\n",
    "    best_num_neurons2 = num_neurons2_bits.uint\n",
    "    best_epoch = epoch_bits.uint\n",
    "    best_batch_size = batch_size_bits.uint\n",
    "    temp = learning_rate_bits.uint\n",
    "    best_learning_rate = temp*(math.exp(-9))\n",
    "    print('\\nNum of neurons1: ', best_num_neurons1, '\\nNum of neurons2: ', best_num_neurons2,  '\\nEpoch:', best_epoch, '\\nBatch_size:', best_batch_size,'\\nLearning rate:',best_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c826a1a",
   "metadata": {},
   "source": [
    "针对这个具体的参数，训练一组网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8058bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and validation (80/20)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_train_scaled = min_max_scaler.fit_transform(data)\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_train_scaled,encoded_y,test_size=0.2)\n",
    "\n",
    "#converting the input train and test set to array format\n",
    "x_train=np.array(x_train)\n",
    "x_test=np.array(x_test)\n",
    "\n",
    "#reshape input data according to LSTM model requirements\n",
    "x_train_modified = x_train.reshape(x_train.shape[0],1,x_train.shape[1])\n",
    "x_test_modified = x_test.reshape(x_test.shape[0],1,x_test.shape[1])\n",
    "\n",
    "'''#rmse function implementation\n",
    "from keras import backend\n",
    "def rmse (y_test, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_test), axis=-1))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5846616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cu_dnnlstm_132 (CuDNNLSTM)  (None, 1, 386)            600616    \n",
      "                                                                 \n",
      " dropout_132 (Dropout)       (None, 1, 386)            0         \n",
      "                                                                 \n",
      " cu_dnnlstm_133 (CuDNNLSTM)  (None, 183)               417972    \n",
      "                                                                 \n",
      " dropout_133 (Dropout)       (None, 183)               0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 1)                 184       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,018,772\n",
      "Trainable params: 1,018,772\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x273e549f070>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Train model using the best individual obtained from the genetic algorithm  \n",
    "optimizer=optimizers.adam_v2.Adam(learning_rate=best_learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(best_num_neurons1, input_shape=(x_train_modified.shape[1],x_train_modified.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(CuDNNLSTM(best_num_neurons2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_train.shape[1])) \n",
    "model.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "#还需要看是否需要切分验证集。如果有划分验证集，则his中的参数就存在val系列\n",
    "history=model.fit(x_train_modified, y_train, epochs=best_epoch, batch_size=best_batch_size,verbose=0)\n",
    "\n",
    "loss = history.history['loss']\n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "460995ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35031629]\n",
      " [0.34781456]\n",
      " [0.85200481]\n",
      " [0.49935183]\n",
      " [0.42755274]\n",
      " [0.51371408]\n",
      " [0.66620405]\n",
      " [0.68906135]\n",
      " [0.45390293]\n",
      " [0.44208886]\n",
      " [0.5587065 ]\n",
      " [0.44199957]\n",
      " [0.48526108]\n",
      " [0.57747506]\n",
      " [0.51325278]\n",
      " [0.44877856]\n",
      " [0.57753368]\n",
      " [0.41591379]\n",
      " [0.42504373]\n",
      " [0.49898806]\n",
      " [0.99913574]\n",
      " [0.5602538 ]\n",
      " [0.71959564]\n",
      " [0.48231815]\n",
      " [0.47142874]\n",
      " [0.44750029]\n",
      " [0.62085567]\n",
      " [0.55813853]\n",
      " [0.35290229]\n",
      " [0.99630266]\n",
      " [0.46417958]\n",
      " [0.49250207]\n",
      " [0.85703779]\n",
      " [0.4865302 ]\n",
      " [0.41170441]\n",
      " [0.59891439]\n",
      " [0.3303919 ]\n",
      " [0.36454005]\n",
      " [0.35992462]\n",
      " [0.14288437]\n",
      " [0.50058977]\n",
      " [0.98933333]\n",
      " [0.47386115]\n",
      " [0.43368415]\n",
      " [0.56211907]\n",
      " [0.99890934]\n",
      " [0.37215293]\n",
      " [0.41902607]\n",
      " [0.66201311]\n",
      " [0.86969814]\n",
      " [0.42995021]\n",
      " [0.6134948 ]\n",
      " [0.55521015]\n",
      " [0.36094166]\n",
      " [0.35210591]\n",
      " [0.6128623 ]\n",
      " [0.37231208]\n",
      " [1.        ]\n",
      " [0.99448014]\n",
      " [0.9902679 ]\n",
      " [0.99927923]\n",
      " [0.41804754]\n",
      " [0.42660093]\n",
      " [0.39675101]\n",
      " [0.98950605]\n",
      " [0.396457  ]\n",
      " [0.55186376]\n",
      " [0.46490163]\n",
      " [0.49105611]\n",
      " [0.99019099]\n",
      " [0.99760515]\n",
      " [0.99101942]\n",
      " [0.34489044]\n",
      " [0.82399134]\n",
      " [0.65523108]\n",
      " [0.42249716]\n",
      " [0.56975003]\n",
      " [0.51337093]\n",
      " [0.3754223 ]\n",
      " [0.36200088]\n",
      " [0.99502223]\n",
      " [0.56415143]\n",
      " [0.40736967]\n",
      " [0.49501218]\n",
      " [0.46311312]\n",
      " [0.99978815]\n",
      " [0.29668976]\n",
      " [0.99973056]\n",
      " [0.56471431]\n",
      " [0.58831483]\n",
      " [0.3588944 ]\n",
      " [0.99858798]\n",
      " [0.99404685]\n",
      " [0.22635622]\n",
      " [0.42009164]\n",
      " [0.53064299]\n",
      " [0.16095775]\n",
      " [0.70582551]\n",
      " [0.99883103]\n",
      " [0.34792769]\n",
      " [0.48871692]\n",
      " [0.4001624 ]\n",
      " [0.52373244]\n",
      " [0.51463494]\n",
      " [0.25368621]\n",
      " [0.43172203]\n",
      " [0.99721777]\n",
      " [0.44379794]\n",
      " [0.45805408]\n",
      " [0.41867802]\n",
      " [0.4108679 ]\n",
      " [0.99598729]\n",
      " [0.45011429]\n",
      " [0.39411986]\n",
      " [0.35053862]\n",
      " [0.58350118]\n",
      " [0.36821876]\n",
      " [0.55549561]\n",
      " [0.46298806]\n",
      " [0.48933943]\n",
      " [0.24351036]\n",
      " [0.44380008]\n",
      " [0.72365571]\n",
      " [0.99691876]\n",
      " [0.99741318]\n",
      " [0.50522323]\n",
      " [0.73409645]\n",
      " [0.44694616]\n",
      " [0.10840647]\n",
      " [0.55302736]\n",
      " [0.21665841]\n",
      " [0.53958048]\n",
      " [0.48418768]\n",
      " [0.99093103]\n",
      " [0.41447459]\n",
      " [0.63279674]\n",
      " [0.98989801]\n",
      " [0.59117223]\n",
      " [0.27160211]\n",
      " [0.28780216]\n",
      " [0.5864258 ]\n",
      " [0.13330324]\n",
      " [0.19817876]\n",
      " [0.68349916]]\n",
      "[[0.34779364]\n",
      " [0.34547016]\n",
      " [0.85506797]\n",
      " [0.489909  ]\n",
      " [0.42053598]\n",
      " [0.50398856]\n",
      " [0.65763646]\n",
      " [0.681321  ]\n",
      " [0.4457997 ]\n",
      " [0.4344446 ]\n",
      " [0.54853237]\n",
      " [0.43435892]\n",
      " [0.4761612 ]\n",
      " [0.5673094 ]\n",
      " [0.5035353 ]\n",
      " [0.44086874]\n",
      " [0.56736827]\n",
      " [0.40944925]\n",
      " [0.41814226]\n",
      " [0.48955324]\n",
      " [1.0192704 ]\n",
      " [0.55007595]\n",
      " [0.7132256 ]\n",
      " [0.4732981 ]\n",
      " [0.4627288 ]\n",
      " [0.43964016]\n",
      " [0.61115044]\n",
      " [0.5479659 ]\n",
      " [0.35019764]\n",
      " [1.0160439 ]\n",
      " [0.45571423]\n",
      " [0.48321784]\n",
      " [0.86057097]\n",
      " [0.4773968 ]\n",
      " [0.40545058]\n",
      " [0.58889943]\n",
      " [0.3293453 ]\n",
      " [0.36104316]\n",
      " [0.3567366 ]\n",
      " [0.16207016]\n",
      " [0.49111992]\n",
      " [1.0081173 ]\n",
      " [0.46508637]\n",
      " [0.42639425]\n",
      " [0.5519378 ]\n",
      " [1.0190127 ]\n",
      " [0.36816174]\n",
      " [0.4124095 ]\n",
      " [0.65331227]\n",
      " [0.8744494 ]\n",
      " [0.42282516]\n",
      " [0.6036681 ]\n",
      " [0.5450471 ]\n",
      " [0.35768497]\n",
      " [0.3494571 ]\n",
      " [0.60302603]\n",
      " [0.36831075]\n",
      " [1.0202552 ]\n",
      " [1.0139695 ]\n",
      " [1.0091791 ]\n",
      " [1.019434  ]\n",
      " [0.4114785 ]\n",
      " [0.41962764]\n",
      " [0.3912924 ]\n",
      " [1.0083134 ]\n",
      " [0.3910148 ]\n",
      " [0.5417151 ]\n",
      " [0.4564122 ]\n",
      " [0.48180735]\n",
      " [1.0090919 ]\n",
      " [1.0175269 ]\n",
      " [1.0100337 ]\n",
      " [0.34275693]\n",
      " [0.8245874 ]\n",
      " [0.6463267 ]\n",
      " [0.41571486]\n",
      " [0.5595669 ]\n",
      " [0.5036513 ]\n",
      " [0.3712247 ]\n",
      " [0.35867304]\n",
      " [1.0145866 ]\n",
      " [0.55396783]\n",
      " [0.40133885]\n",
      " [0.485668  ]\n",
      " [0.45468378]\n",
      " [1.0200138 ]\n",
      " [0.29843515]\n",
      " [1.0199482 ]\n",
      " [0.5545304 ]\n",
      " [0.5782066 ]\n",
      " [0.35577625]\n",
      " [1.0186465 ]\n",
      " [1.0134766 ]\n",
      " [0.23512208]\n",
      " [0.41342387]\n",
      " [0.5206709 ]\n",
      " [0.17769568]\n",
      " [0.69879997]\n",
      " [1.0189232 ]\n",
      " [0.34557518]\n",
      " [0.47952694]\n",
      " [0.39451596]\n",
      " [0.5138496 ]\n",
      " [0.50489366]\n",
      " [0.25953257]\n",
      " [0.42451817]\n",
      " [1.0170858 ]\n",
      " [0.4360845 ]\n",
      " [0.44980043]\n",
      " [0.41207832]\n",
      " [0.40465662]\n",
      " [1.0156848 ]\n",
      " [0.44215322]\n",
      " [0.38880873]\n",
      " [0.3480003 ]\n",
      " [0.57336277]\n",
      " [0.36448067]\n",
      " [0.5453315 ]\n",
      " [0.45456293]\n",
      " [0.48013362]\n",
      " [0.2504153 ]\n",
      " [0.43608657]\n",
      " [0.71749073]\n",
      " [1.0167453 ]\n",
      " [1.0173084 ]\n",
      " [0.49565664]\n",
      " [0.72848314]\n",
      " [0.43910766]\n",
      " [0.13255501]\n",
      " [0.54287326]\n",
      " [0.22651866]\n",
      " [0.5295161 ]\n",
      " [0.47511658]\n",
      " [1.009933  ]\n",
      " [0.40808147]\n",
      " [0.62332606]\n",
      " [1.008759  ]\n",
      " [0.58108556]\n",
      " [0.27566665]\n",
      " [0.29034564]\n",
      " [0.5763048 ]\n",
      " [0.15382954]\n",
      " [0.21020912]\n",
      " [0.67554194]]\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(x_test)\n",
    "print(x_test)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b83646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
